{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pep8_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext pep8_magic\n"
     ]
    }
   ],
   "source": [
    "%load_ext pep8_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypher = pyphen.Pyphen(lang='hu_HU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyph_tags(word, hypher=pyphen.Pyphen(lang='hu_HU'), aslist=False):\n",
    "    \"\"\"Hyphenating classification of the characters in the word.\n",
    "    {B(egin),M(iddle),E(nd),S(ingle)}\"\"\"\n",
    "    if (len(word) == 0):\n",
    "        raise IndexError(\"0 length word\")\n",
    "    ret = list('M' * len(word))\n",
    "    ret[0] = 'B'\n",
    "    ret[-1] = 'E'\n",
    "    for i in hypher.positions(word):\n",
    "        ret[i] = 'B'\n",
    "        if(ret[i-1] == 'B'):\n",
    "            ret[i-1] = 'S'\n",
    "        else:\n",
    "            ret[i-1] = 'E'\n",
    "    if (aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def hyph_tags_4to2(word, aslist=False):\n",
    "    \"\"\"{B,M,E,S} to {B, M}\"\"\"\n",
    "    ret = list(word)\n",
    "    for i in range(len(ret)):\n",
    "        if ret[i] == 'S':\n",
    "            ret[i] = 'B'\n",
    "        if ret[i] != 'B':\n",
    "            ret[i] = 'M'\n",
    "    if(aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def same_char_num(word, hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    \"\"\"Return true if the hyphenated word has as many chars as the original\"\"\"\n",
    "    return len(hypher.inserted(word)) == len(word)+len(hypher.positions(word))\n",
    "\n",
    "\n",
    "def cleaning(data):\n",
    "    \"\"\"Text cleaning:\n",
    "        lower the letters\n",
    "        punctuation, digits ellimination\"\"\"\n",
    "    formated_data = data.lower()\n",
    "    formated_data = re.sub('['+string.punctuation+']', '', formated_data)\n",
    "    formated_data = re.sub('['+string.digits+']', '', formated_data)\n",
    "    return formated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter_hu_data = collections.Counter()\n",
    "word_list = []\n",
    "c_all = 0\n",
    "c_same_char_num = 0\n",
    "with open('web2.2-freq-sorted.txt', 'r',\n",
    "          errors='ignore', encoding='latin2') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        i = i+1\n",
    "        words = line.split()\n",
    "        if len(words) > 1:\n",
    "            if(words[1].isdigit()):\n",
    "                counter_hu_data[cleaning(words[0])] += int(words[1])\n",
    "        if i > 100500:\n",
    "            break\n",
    "for words in counter_hu_data.most_common(100000):\n",
    "    c_all += 1\n",
    "    next_word = words[0]\n",
    "    if(len(next_word) != 0 and same_char_num(next_word)):\n",
    "        c_same_char_num += 1\n",
    "        word_list.append([next_word, hyph_tags_4to2(hyph_tags(next_word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82563 83678 0.9866751117378523\n"
     ]
    }
   ],
   "source": [
    "print(c_same_char_num, c_all, c_same_char_num/c_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ember', 'BMBMM'], ['ezek', 'BMMM'], ['tovább', 'BMBMMM'], ['vagyok', 'BMBMMM'], ['ma', 'BM'], ['miatt', 'BMBMM'], ['this', 'BMMM'], ['mellett', 'BMMBMMM'], ['vannak', 'BMMBMM'], ['by', 'BM'], ['alapján', 'BMMMBMM'], ['lenne', 'BMMBM'], ['with', 'BMMM'], ['that', 'BMMM'], ['tehát', 'BMBMM'], ['te', 'BM'], ['it', 'BM'], ['ben', 'BMM'], ['egész', 'BMMMM'], ['néhány', 'BMBMMM'], ['milyen', 'BMBMMM'], ['át', 'BM'], ['nekem', 'BMBMM'], ['előtt', 'BMMMM'], ['ezzel', 'BMBMM'], ['mivel', 'BMBMM'], ['ezen', 'BMMM'], ['nélkül', 'BMMBMM'], ['lett', 'BMMM'], ['stb', 'BMM'], ['you', 'BMM'], ['viszont', 'BMBMMMM'], ['év', 'BM'], ['teljes', 'BMMBMM'], ['erre', 'BMBM'], ['hiszen', 'BMBMMM'], ['másik', 'BMBMM'], ['ebben', 'BMBMM'], ['mind', 'BMMM'], ['der', 'BMM'], ['három', 'BMBMM'], ['talán', 'BMBMM'], ['valami', 'BMBMBM'], ['die', 'BMM'], ['bár', 'BMM'], ['are', 'BMM'], ['legyen', 'BMBMMM'], ['tudom', 'BMBMM'], ['from', 'BMMM'], ['inkább', 'BMBMMM'], ['során', 'BMBMM'], ['például', 'BMMBMBM'], ['rá', 'BM'], ['m', 'M'], ['persze', 'BMMBMM'], ['pl', 'BM'], ['mely', 'BMMM'], ['azok', 'BMMM'], ['áll', 'BMM'], ['szerintem', 'BMMBMMBMM'], ['miért', 'BMBMM'], ['jelenleg', 'BMBMMBMM'], ['aztán', 'BMBMM'], ['c', 'M'], ['na', 'BM'], ['ft', 'BM'], ['európai', 'BMBMBMM'], ['nagyobb', 'BMBMMMM'], ['adott', 'BMMMM'], ['akár', 'BMMM'], ['elég', 'BMMM'], ['se', 'BM'], ['éves', 'BMMM'], ['fontos', 'BMMBMM'], ['amelyek', 'BMMBMMM'], ['együtt', 'BMMMMM'], ['éppen', 'BMBMM'], ['szükséges', 'BMMMBMBMM'], ['hát', 'BMM'], ['megfelelő', 'BMMBMBMBM'], ['őket', 'BMMM'], ['neki', 'BMBM'], ['szó', 'BMM'], ['program', 'BMMMBMM'], ['nap', 'BMM'], ['jobb', 'BMMM'], ['további', 'BMBMMBM'], ['ahogy', 'BMMMM'], ['teljesen', 'BMMBMBMM'], ['hozzá', 'BMMBM'], ['következő', 'BMBMMBMBM'], ['maga', 'BMBM'], ['or', 'BM'], ['rt', 'BM'], ['egyes', 'BMMMM'], ['benne', 'BMMBM'], ['esetén', 'BMMBMM'], ['különböző', 'BMBMMBMBM'], ['belül', 'BMBMM'], ['all', 'BMM']]\n"
     ]
    }
   ],
   "source": [
    "print(word_list[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# onehot: {'B','M','E','S'}\n",
    "def one_hot_encode(char, dictionary='BMES'):\n",
    "    ret = [0]*len(dictionary)\n",
    "    if char in dictionary:\n",
    "        ret[dictionary.find(char)] = 1\n",
    "        return ret\n",
    "    raise ValueError('Value out of dictionary range: '+char)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    \"\"\"Randomize 2 same length array in the same permutation\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def one_hot_decode(arr, dictionary='BMES'):\n",
    "    assert len(arr) == len(dictionary)\n",
    "    i = np.nonzero(arr)[0][0]\n",
    "    return dictionary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hun_chars = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz' + '^$'  # ^,$\n",
    "\n",
    "\n",
    "def generate_network_data(data, ret_input=[], ret_output=[],\n",
    "                          length=2, length_after=0,\n",
    "                          start_char='^', end_char='$',\n",
    "                          chars=hun_chars, tag_chars='BMES'):\n",
    "    \"\"\"from [word,hyph_class(word) to length-long input-output data\"\"\"\n",
    "    word = data[0]\n",
    "    word_plus = start_char*(length-length_after-1)+word+end_char*length_after\n",
    "    hyph_word = data[1]\n",
    "    for i in range(0, len(word)):\n",
    "        input_next_iter = []\n",
    "        for c in word_plus[i:i+length]:\n",
    "            input_next_iter.append(one_hot_encode(c, chars))\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] [1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "wc_in = []\n",
    "wc_out = []\n",
    "\n",
    "generate_network_data(['alma', 'BEBE'], wc_in, wc_out, 3, 1)\n",
    "print(wc_in[0], wc_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "length_after = 2\n",
    "tag_chars = 'BM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data len:  698570\n",
      "Wrong words:  646\n"
     ]
    }
   ],
   "source": [
    "data_in = []\n",
    "data_out = []\n",
    "wrong_word = 0\n",
    "for word in word_list:\n",
    "    try:\n",
    "        generate_network_data(word, data_in, data_out,\n",
    "                              window_length, tag_chars=tag_chars,\n",
    "                              length_after=length_after)\n",
    "    except ValueError:\n",
    "        wrong_word += 1\n",
    "print('Data len: ', len(data_in))\n",
    "print('Wrong words: ', wrong_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (488999, 5, 37) (488999, 2)\n",
      "Validation data size: (139714, 5, 37) (139714, 2)\n",
      "Test data size: (69857, 5, 37) (69857, 2)\n"
     ]
    }
   ],
   "source": [
    "valid_rate = 0.2\n",
    "test_rate = 0.1\n",
    "data_len = len(data_in)\n",
    "\n",
    "data_in = np.array(data_in, dtype='float32')\n",
    "data_out = np.array(data_out, dtype='float32')\n",
    "data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "tests_input = data_in[0:int(data_len*test_rate)]\n",
    "tests_target = data_out[0:int(data_len*test_rate)]\n",
    "valid_input = data_in[int(data_len*test_rate):\n",
    "                      int(data_len*(test_rate+valid_rate))]\n",
    "valid_target = data_out[int(data_len*test_rate):\n",
    "                        int(data_len*(test_rate+valid_rate))]\n",
    "train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "\n",
    "print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "print('Validation data size:', np.shape(valid_input), np.shape(valid_target))\n",
    "print('Test data size:', np.shape(tests_input), np.shape(tests_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_flatten = np.reshape(\n",
    "    train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "valid_input_flatten = np.reshape(\n",
    "    valid_input, (len(valid_input), (window_length)*len(hun_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=((window_length)*len(hun_chars)),\n",
    "                units=10, name='input_layer', activation='sigmoid'))\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(2, name='output_layer', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488999, 185)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_input_flatten, train_target,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_flatten, valid_target),\n",
    "                    verbose=0, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.ylim((0.04, 0.06))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(history.history['val_loss'][-10:], history.epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hpyh_predict(word, model,\n",
    "                 length=2, length_after=0, tag_chars='BMES', aslist=False):\n",
    "    \"\"\"Generate tagging from the input word according to the model\"\"\"\n",
    "    word_in = []\n",
    "    word_out = []\n",
    "    generate_network_data([word, len(word)*tag_chars[0]],\n",
    "                          word_in, word_out, length=length,\n",
    "                          length_after=length_after, tag_chars=tag_chars)\n",
    "    word_in = np.reshape(word_in, (len(word_in), (length)*len(hun_chars)))\n",
    "    word_out = model.predict(word_in)\n",
    "    tag_list = np.array(list(tag_chars))\n",
    "    temp = np.argmax(word_out, axis=1)\n",
    "    temp = tag_list[temp]\n",
    "    if(aslist):\n",
    "        return temp\n",
    "    return \"\".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: szemüveg Prediction: BMMBMBMM Target: BMMMBMMM szem-üveg\n",
      "Word: leopárd Prediction: BMBBMMM Target: BMBBMMM le-o-párd\n"
     ]
    }
   ],
   "source": [
    "test = 'szemüveg'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hpyh_predict(test, model, window_length, length_after, tag_chars),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))\n",
    "\n",
    "test = 'leopárd'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hpyh_predict(test, model, window_length, length_after, tag_chars),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = counter_hu_data.most_common()[-500:]\n",
    "print('Word\\tSuccess\\tPrediction\\tTarget\\tVisualization')\n",
    "for word in test_words:\n",
    "    next_word = word[0]\n",
    "    if(len(next_word) != 0 and same_char_num(next_word)):\n",
    "        try:\n",
    "            predicted_value = hpyh_predict(next_word, model,\n",
    "                                           window_length, length_after,\n",
    "                                           tag_chars)\n",
    "            excepted_value = hyph_tags_4to2(hyph_tags(next_word))\n",
    "            success = predicted_value == excepted_value\n",
    "            if not success:\n",
    "                print(next_word, '\\t', success, '\\t', predicted_value,\n",
    "                      '\\t', excepted_value, '\\t', hypher.inserted(next_word))\n",
    "        except ValueError as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
