{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, Embedding, LSTM\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext pep8_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hun_chars = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz' + '^$'  # ^,$\n",
    "\n",
    "\n",
    "def hyph_tags(word, hypher=pyphen.Pyphen(lang='hu_HU'), aslist=False):\n",
    "    \"\"\"Hyphenating classification of the characters in the word.\n",
    "    {B(egin),M(iddle),E(nd),S(ingle)}\"\"\"\n",
    "    if (len(word) == 0):\n",
    "        raise IndexError(\"0 length word\")\n",
    "    ret = list('M' * len(word))\n",
    "    ret[0] = 'B'\n",
    "    ret[-1] = 'E'\n",
    "    for i in hypher.positions(word):\n",
    "        ret[i] = 'B'\n",
    "        if(ret[i-1] == 'B'):\n",
    "            ret[i-1] = 'S'\n",
    "        else:\n",
    "            ret[i-1] = 'E'\n",
    "    if (aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def hyph_tags_4to2(word, aslist=False):\n",
    "    \"\"\"{B,M,E,S} to {B, M}\"\"\"\n",
    "    ret = list(word)\n",
    "    for i in range(len(ret)):\n",
    "        if ret[i] == 'S':\n",
    "            ret[i] = 'B'\n",
    "        if ret[i] != 'B':\n",
    "            ret[i] = 'M'\n",
    "    if(aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def same_char_num(word, hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    \"\"\"Return true if the hyphenated word has as many chars as the original\"\"\"\n",
    "    return len(hypher.inserted(word)) == len(word)+len(hypher.positions(word))\n",
    "\n",
    "\n",
    "def cleaning(data):\n",
    "    \"\"\"Text cleaning:\n",
    "        lower the letters\n",
    "        punctuation, digits ellimination\"\"\"\n",
    "    formated_data = data.lower()\n",
    "    formated_data = re.sub('['+string.punctuation+']', '', formated_data)\n",
    "    formated_data = re.sub('['+string.digits+']', '', formated_data)\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# onehot: {'B','M','E','S'}\n",
    "def one_hot_encode(char, dictionary='BMES'):\n",
    "    ret = [0]*len(dictionary)\n",
    "    if char in dictionary:\n",
    "        ret[dictionary.find(char)] = 1\n",
    "        return ret\n",
    "    raise ValueError('Value out of dictionary range: '+char)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    \"\"\"Randomize 2 same length array in the same permutation\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def one_hot_decode(arr, dictionary='BMES'):\n",
    "    assert len(arr) == len(dictionary)\n",
    "    i = np.nonzero(arr)[0][0]\n",
    "    return dictionary[i]\n",
    "\n",
    "\n",
    "def generate_network_data(data, ret_input=[], ret_output=[],\n",
    "                          length=2, length_after=0,\n",
    "                          start_char='^', end_char='$',\n",
    "                          chars=hun_chars, tag_chars='BMES'):\n",
    "    \"\"\"from [word,hyph_class(word) to length-long input-output data\"\"\"\n",
    "    word = data[0]\n",
    "    word_plus = start_char*(length-length_after-1)+word+end_char*length_after\n",
    "    hyph_word = data[1]\n",
    "    for i in range(0, len(word)):\n",
    "        input_next_iter = []\n",
    "        for c in word_plus[i:i+length]:\n",
    "            input_next_iter.append(one_hot_encode(c, chars))\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return\n",
    "\n",
    "def generate_network_words(data, padding=None, start_char='^',\n",
    "                           end_char='$', chars=hun_chars,\n",
    "                           tag_chars='BMES', tag_default=-1):\n",
    "    \"\"\"One-hot [word, hyph_class(word)]->[[[010],[010]],[[01],[01]]]\n",
    "    padding to fixed size, if not null\"\"\"\n",
    "    ret_input=[]\n",
    "    ret_output=[]\n",
    "    \n",
    "    word = data[0]\n",
    "    hyph_word = data[1]\n",
    "    if padding != None:\n",
    "        if len(word)>padding:\n",
    "            raise IndexError(\"The word is longer than the fixed size\")\n",
    "        else:\n",
    "            word = word + (padding-len(word))*end_char\n",
    "            hyph_word = hyph_word + (padding-len(hyph_word)) * tag_chars[tag_default]\n",
    "    for i in range(0,len(word)):\n",
    "        input_next_iter = one_hot_encode(word[i],chars)\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return ret_input, ret_output\n",
    "    \n",
    "def hyph_tupples(data, hypher=pyphen.Pyphen(lang='hu_HU'),\n",
    "                tag_chars='BM'):\n",
    "    \"\"\"[words] -> [words, hyph_words]\"\"\"\n",
    "    word_list = []\n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    for next_word in data:\n",
    "        c_all += 1\n",
    "        if(len(next_word) != 0 and same_char_num(next_word, hypher)):\n",
    "            c_same_char_num += 1\n",
    "            if(len(tag_chars) == 2):\n",
    "                word_list.append([next_word,\n",
    "                                  hyph_tags_4to2(hyph_tags(next_word))])\n",
    "            else:\n",
    "                word_list.append([next_word, hyph_tags(next_word)])\n",
    "    return word_list, c_all, c_same_char_num\n",
    "\n",
    "def tupple_to_train(word_list, window_length, length_after,\n",
    "                 tag_chars='BM'):\n",
    "    \"\"\"[words, hyph_words] -> in[0,1,0...], out[0,1,0...]\"\"\"\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            generate_network_data(word, data_in, data_out,\n",
    "                                  window_length, tag_chars=tag_chars,\n",
    "                                  length_after=length_after)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "    return data_in, data_out, wrong_word\n",
    "\n",
    "def bigram_counter_from_file(filename):\n",
    "    \"\"\"creates bigram counter from file\"\"\"\n",
    "    with open(filename) as f:\n",
    "        word_list = []\n",
    "        for words in f:\n",
    "            words = words.strip()\n",
    "            words = words.split()\n",
    "            for w in words:\n",
    "                w = cleaning(w)\n",
    "                if len(w)>0:\n",
    "                    word_list.append(w)\n",
    "\n",
    "    bigram_counter = collections.Counter()\n",
    "    for word in word_list:\n",
    "        for i in range(2,len(word)):\n",
    "            bigram_counter[word[i-2:i]] += 1\n",
    "    return bigram_counter\n",
    "\n",
    "def bigrams_in_word(word, bigram_counter, mc=100):\n",
    "    bigrams = np.array(bigram_counter.most_common(mc))[:,0]\n",
    "    w_bc = len(word)-1\n",
    "    if w_bc<1:\n",
    "        return 1.0\n",
    "    w_bf = 0\n",
    "    for i in range(2,len(word)):\n",
    "        if word[i-2:i] in bigrams:\n",
    "            w_bf +=1\n",
    "    return w_bf/w_bc\n",
    "\n",
    "def bigram_selector(word, bigram_counters,threshold=0.2, mc=100):\n",
    "    \"\"\"Choose the language of the word\"\"\"\n",
    "    lang_likes = np.zeros(len(bigram_counters)+1)\n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        lang_likes[i] = bigrams_in_word(word, bigram_counters[i], mc)\n",
    "    lang_likes_max = np.argmax(lang_likes)\n",
    "    \n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        if i!=lang_likes_max:\n",
    "            if lang_likes[lang_likes_max]-lang_likes[i]<=threshold:\n",
    "                return len(bigram_counters)\n",
    "    return lang_likes_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_reader(file, tail_cut=100000,\n",
    "                lang_selector = False, lang_thr=0.6,\n",
    "                lang_file_en='../wikipedia/angol/ossz_angol',\n",
    "                lang_file_hu='../wikipedia/magyar/ossz_magyar'):\n",
    "    \"\"\"Read data from file\"\"\"\n",
    "\n",
    "    if lang_selector:\n",
    "        bigram_counter_en = bigram_counter_from_file(lang_file_en)\n",
    "        bigram_counter_hu = bigram_counter_from_file(lang_file_hu)\n",
    "        out_en_words = 0\n",
    "    \n",
    "    tail_cut_ptest_words = tail_cut + 500\n",
    "\n",
    "    counter_hu_data = collections.Counter()\n",
    "    with open(file, 'r',\n",
    "              errors='ignore', encoding='latin2') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i = i+1\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if(words[1].isdigit()):\n",
    "                    cword = cleaning(words[0])\n",
    "                    if lang_selector:\n",
    "                        lang = bigram_selector(cword,\n",
    "                                            [bigram_counter_hu,\n",
    "                                             bigram_counter_en],\n",
    "                                            lang_thr)\n",
    "                        if (lang!=1):\n",
    "                            counter_hu_data[cword] += int(words[1])\n",
    "                        else:\n",
    "                            out_en_words +=1\n",
    "                    else:\n",
    "                        counter_hu_data[cword] += int(words[1])\n",
    "            if i > tail_cut_ptest_words:\n",
    "                break\n",
    "    if lang_selector:\n",
    "        print(\"Throwed english words: \", out_en_words)\n",
    "    return counter_hu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_generator(data_counter, window_length, length_after,\n",
    "                         tag_chars='BM', tail_cut=100000,\n",
    "                         valid_rate=0.2, test_rate=0.1,\n",
    "                         language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "    unique words -> characters -> randomize -> cut\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                    hypher=hypher)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "\n",
    "\n",
    "def train_data_generator_uwords(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1,\n",
    "                                language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> randomize -> cut -> characters\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    np.random.shuffle(data_list)\n",
    "    data_len = len(data_list)\n",
    "    tests_data = data_list[0:int(data_len*test_rate)]\n",
    "    valid_data = data_list[int(data_len*test_rate):\n",
    "                           int(data_len*(test_rate+valid_rate))]\n",
    "    train_data = data_list[int(data_len*(test_rate+valid_rate)):]\n",
    "    \n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    tests_list, c_all_p, c_same_char_num_p = hyph_tupples(tests_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    valid_list, c_all_p, c_same_char_num_p = hyph_tupples(valid_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    train_list, c_all_p, c_same_char_num_p = hyph_tupples(train_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    \n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    wrong_word = 0\n",
    "    tests_input, tests_target, wrong_w_p = tupple_to_train(tests_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    valid_input, valid_target, wrong_w_p = tupple_to_train(valid_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    train_input, train_target, wrong_w_p = tupple_to_train(train_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    \n",
    "\n",
    "def train_data_generator_uchars(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1,\n",
    "                                language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> characters -> unique -> randomize -> cut\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                     hypher=hypher)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    #Unique\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    \n",
    "    shape_in = np.shape(data_in)\n",
    "    shape_out = np.shape(data_out)\n",
    "    \n",
    "    data_in_flatten = np.reshape(\n",
    "        data_in, (shape_in[0], shape_in[1]*shape_in[2]))\n",
    "    shape_in_flatten = np.shape(data_in_flatten)\n",
    "    \n",
    "    data_iosum = np.concatenate((data_in_flatten, data_out), axis=1)\n",
    "    data_iosum_unique = np.vstack({tuple(row) for row in data_iosum})\n",
    "    \n",
    "    data_in = data_iosum_unique[:,:-shape_out[1]]\n",
    "    data_out = data_iosum_unique[:,-shape_out[1]:]\n",
    "    print('Data unique len: ', np.shape(data_iosum_unique)[0])\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_words(data_counter, tag_chars='BM', padding = 30, tail_cut=100000,\n",
    "                     valid_rate=0.2, test_rate=0.1,\n",
    "                     language='hu'):\n",
    "    \"\"\"Training data, example: alma -> {[[1,0..][0,0..][0,0...][0,0...]],[[1,0],[0,1][1,0][0,1]]}\"\"\"\n",
    "    \n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "        \n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                    hypher=hypher)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    data_words = []\n",
    "    wrong_word = 0\n",
    "    long_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            next_data_in, next_data_out = generate_network_words(word, padding = padding, tag_chars=tag_chars)\n",
    "            next_data_in = np.array(next_data_in, dtype='float32')\n",
    "            next_data_out = np.array(next_data_out, dtype='float32')\n",
    "            data_in.append(next_data_in)\n",
    "            data_out.append(next_data_out)\n",
    "            data_words.append(word)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "        except IndexError:\n",
    "            long_word += 1\n",
    "            print(word)\n",
    "            \n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "    print('Words longer than the padding: ', long_word)\n",
    "    \n",
    "    data_in = np.array(data_in)\n",
    "    data_out = np.array(data_out)\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    order = np.random.permutation(data_len)\n",
    "    data_in = [data_in[k] for k in order]\n",
    "    data_out = [data_out[k] for k in order]\n",
    "    data_words = [data_words[k] for k in order]\n",
    "    \n",
    "    #data_in, data_out, word_list = unison_shuffled_copies(data_in, data_out, word_list)\n",
    "    \n",
    "    datas = {}\n",
    "    \n",
    "    datas[\"tests_words\"] = np.array(data_words[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_input\"] = np.array(data_in[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_target\"] = np.array(data_out[0:int(data_len*test_rate)])\n",
    "    datas[\"valid_words\"] = np.array(data_words[int(data_len*test_rate):\n",
    "                                               int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_input\"] = np.array(data_in[int(data_len*test_rate):\n",
    "                                            int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_target\"] = np.array(data_out[int(data_len*test_rate):\n",
    "                                              int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"train_words\"] = np.array(data_words[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_input\"] = np.array(data_in[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_target\"] = np.array(data_out[int(data_len*(test_rate+valid_rate)):])\n",
    "    \n",
    "    return datas, wrong_word, long_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_creator_dnn(window_length, output_length, num_layers=1,\n",
    "                  num_hidden=10, chars=hun_chars):\n",
    "    \"\"\"Creates Keras model with the given input, output dimensions\n",
    "    and layer number, hidden layer length\"\"\"\n",
    "    \n",
    "    input_shape = window_length*len(chars)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(input_dim=(input_shape),\n",
    "                    units=num_hidden, name='input_layer',\n",
    "                    activation='sigmoid'))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(units=num_hidden, activation='sigmoid'))\n",
    "\n",
    "    # model.add(Flatten())\n",
    "    model.add(Dense(output_length, name='output_layer', activation='softmax'))\n",
    "\n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_cnn(output_length,\n",
    "                      num_layers=1, num_hidden=516,\n",
    "                      kernel_size=10, strides=1, word_length = 30, chars=hun_chars):\n",
    "    \"\"\"Creates Keras CNN model\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(num_hidden,kernel_size, strides=strides, padding=\"same\",\n",
    "                     activation='relu', input_shape=(word_length, len(chars))))\n",
    "    for i in range(1,num_layers):\n",
    "        model.add(Conv1D(num_hidden,kernel_size,strides=strides,\n",
    "                         padding=\"same\", activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_lstm(output_length, \n",
    "                       num_layers=2, num_hidden=64,\n",
    "                       word_length = 30, chars = hun_chars):\n",
    "    \"\"\"Creates Keras LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(num_hidden, activation='relu',return_sequences=True,\n",
    "                   go_backwards=True,\n",
    "                   input_shape=(word_length, len(chars))))\n",
    "    for i in range(1,num_layers):\n",
    "        model.add(LSTM(num_hidden, activation='relu',return_sequences=True,\n",
    "                       go_backwards=True,))\n",
    "    \n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully\n",
      "non-standard hyphenation:\n",
      "82563 83678 0.9866751117378523\n",
      "['mailtomaiserszechenyinkzsasulinethu', 'BMMMBMBMMMMMMMBMBMBMMMMBMMBMBMBMMBM']\n",
      "['httpdelphiszechenyinkzsasulinethu', 'BMMMBMMMBMMMBMBMBMMMMBMMBMBMBMMBM']\n",
      "['ftpftpszechenyinkzsasulinethudelphi', 'BMMMMMMMBMBMBMMMMBMMBMBMBMMBMBMMMBM']\n",
      "['httpwwwegyismertszerverhuképgif', 'BMMMMMBMMMBMBMMMBMMMBMMBMBMMBMM']\n",
      "['orgapachecatalinacorestandardpipeline', 'BMBMMMMMMBMBMBMBMBMBMMMMMBMMMBMBMBMMM']\n",
      "['standardpipelinevalvecontextinvokenext', 'BMMMBMMMBMBMBMBMBMMBMBMMBMMBMMBMBMMBMM']\n",
      "['httpwwwsomeunknownplacenetmypicturegif', 'BMMMMMMBMMBMMMBMMMMMMBMBMMMMBMMBMBMBMM']\n",
      "Data len:  81910\n",
      "Words with unrecognized caracter:  646\n",
      "Words longer than the padding:  7\n",
      "(57337, 30, 37) (16382, 30, 37) (8191, 30, 37)\n"
     ]
    }
   ],
   "source": [
    "padding = 30\n",
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 3\n",
    "num_hidden = 150\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "\n",
    "datas, wrong_words, long_word = train_data_words(counter_hu_data, tag_chars,\n",
    "                                                 padding, tail_cut, language='hu')\n",
    "\n",
    "tests_input_cnn = datas[\"tests_input\"]\n",
    "tests_target_cnn = datas[\"tests_target\"]\n",
    "valid_input_cnn = datas[\"valid_input\"]\n",
    "valid_target_cnn = datas[\"valid_target\"]\n",
    "train_input_cnn = datas[\"train_input\"]\n",
    "train_target_cnn = datas[\"train_target\"]\n",
    "\n",
    "tests_words = datas[\"tests_words\"]\n",
    "valid_words = datas[\"valid_words\"]\n",
    "train_words = datas[\"train_words\"]\n",
    "\n",
    "\n",
    "print(np.shape(train_input_cnn), np.shape(valid_input_cnn), np.shape(tests_input_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still wrong word (expected zero):  0\n"
     ]
    }
   ],
   "source": [
    "wrong_word = 0\n",
    "tests_input, tests_target, wrong_w_p = tupple_to_train(tests_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "valid_input, valid_target, wrong_w_p = tupple_to_train(valid_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "train_input, train_target, wrong_w_p = tupple_to_train(train_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "\n",
    "print(\"Still wrong word (expected zero): \", wrong_word)\n",
    "\n",
    "train_input_flatten = np.reshape(\n",
    "    train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "valid_input_flatten = np.reshape(\n",
    "    valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "tests_input_flatten = np.reshape(\n",
    "    tests_input, (len(tests_input), (window_length)*len(hun_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully\n",
      "non-standard hyphenation:\n",
      "82563 83678 0.9866751117378523\n",
      "Data len:  698309\n",
      "Words with unrecognized caracter:  646\n",
      "Training data size: (488817, 7, 37) (488817, 2)\n",
      "Validation data size: (139662, 7, 37) (139662, 2)\n",
      "Test data size: (69830, 7, 37) (69830, 2)\n",
      "Network data generated successfully\n"
     ]
    }
   ],
   "source": [
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 5\n",
    "num_hidden = 110\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "[train_input_flatten, train_target,\n",
    " valid_input_flatten, valid_target,\n",
    " tests_input_flatten,\n",
    " tests_target] = train_data_generator(counter_hu_data,\n",
    "                                             window_length,\n",
    "                                             length_after,\n",
    "                                             tag_chars,\n",
    "                                             tail_cut)\n",
    "\n",
    "#train_input_flatten_p1 = np.expand_dims(train_input_flatten, axis=1) # reshape (X, 1, 259) \n",
    "#valid_input_flatten_p1 = np.expand_dims(valid_input_flatten, axis=1)\n",
    "#tests_input_flatten_p1 = np.expand_dims(tests_input_flatten, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models created. Start training\n"
     ]
    }
   ],
   "source": [
    "# Creating the keras model\n",
    "model_dnn = model_creator_dnn(window_length, len(tag_chars),\n",
    "                      num_layers, num_hidden)\n",
    "\n",
    "earlyStopping_dnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "earlyStopping_cnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "model_lstm = model_creator_lstm(len(tag_chars), num_layers=2, num_hidden=128)\n",
    "\n",
    "earlyStopping_lstm = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "print('Models created. Start training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_lstm, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57337 samples, validate on 16382 samples\n",
      "Epoch 1/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.3362 - val_loss: 0.1985\n",
      "Epoch 2/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1913 - val_loss: 0.1879\n",
      "Epoch 3/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.1824 - val_loss: 0.1787\n",
      "Epoch 4/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1748 - val_loss: 0.1730\n",
      "Epoch 5/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1706 - val_loss: 0.1715\n",
      "Epoch 6/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1754 - val_loss: 0.1775\n",
      "Epoch 7/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1715 - val_loss: 0.1689\n",
      "Epoch 8/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.1574 - val_loss: 0.1416\n",
      "Epoch 9/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1283 - val_loss: 0.1211\n",
      "Epoch 10/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1161 - val_loss: 0.1127\n",
      "Epoch 11/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1075 - val_loss: 0.1317\n",
      "Epoch 12/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.1012 - val_loss: 0.0636\n",
      "Epoch 13/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0559 - val_loss: 0.0487\n",
      "Epoch 14/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0488 - val_loss: 0.0439\n",
      "Epoch 15/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0408 - val_loss: 0.0367\n",
      "Epoch 16/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0341 - val_loss: 0.0314\n",
      "Epoch 17/1000\n",
      "57337/57337 [==============================] - 128s - loss: 0.0293 - val_loss: 0.0272\n",
      "Epoch 18/1000\n",
      "57337/57337 [==============================] - 124s - loss: 0.0259 - val_loss: 0.0245\n",
      "Epoch 19/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0236 - val_loss: 0.0228\n",
      "Epoch 20/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0220 - val_loss: 0.0216\n",
      "Epoch 21/1000\n",
      "57337/57337 [==============================] - 131s - loss: 0.0210 - val_loss: 0.0207\n",
      "Epoch 22/1000\n",
      "57337/57337 [==============================] - 138s - loss: 0.0199 - val_loss: 0.0199\n",
      "Epoch 23/1000\n",
      "57337/57337 [==============================] - 134s - loss: 0.0193 - val_loss: 0.0195\n",
      "Epoch 24/1000\n",
      "57337/57337 [==============================] - 125s - loss: 0.0186 - val_loss: 0.0187\n",
      "Epoch 25/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0185 - val_loss: 0.0186\n",
      "Epoch 26/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0176 - val_loss: 0.0181\n",
      "Epoch 27/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0175 - val_loss: 0.0177\n",
      "Epoch 28/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 29/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0165 - val_loss: 0.0168\n",
      "Epoch 30/1000\n",
      "57337/57337 [==============================] - 124s - loss: 0.0164 - val_loss: 0.0168\n",
      "Epoch 31/1000\n",
      "57337/57337 [==============================] - 124s - loss: 0.0159 - val_loss: 0.0162\n",
      "Epoch 32/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 33/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0154 - val_loss: 0.0164\n",
      "Epoch 34/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0151 - val_loss: 0.0159\n",
      "Epoch 35/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 36/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0144 - val_loss: 0.0159\n",
      "Epoch 37/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0145 - val_loss: 0.0153\n",
      "Epoch 38/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0139 - val_loss: 0.0148\n",
      "Epoch 39/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0143 - val_loss: 0.0152\n",
      "Epoch 40/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0136 - val_loss: 0.0143\n",
      "Epoch 41/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 42/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0130 - val_loss: 0.0142\n",
      "Epoch 43/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0127 - val_loss: 0.0135\n",
      "Epoch 44/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0123 - val_loss: 0.0137\n",
      "Epoch 45/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0123 - val_loss: 0.0134\n",
      "Epoch 46/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0124 - val_loss: 0.0139\n",
      "Epoch 47/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0118 - val_loss: 0.0128\n",
      "Epoch 48/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 49/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 50/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0113 - val_loss: 0.0129\n",
      "Epoch 51/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 52/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 53/1000\n",
      "57337/57337 [==============================] - 123s - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 54/1000\n",
      "57337/57337 [==============================] - 124s - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 55/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 56/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 57/1000\n",
      "57337/57337 [==============================] - 127s - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 58/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0094 - val_loss: 0.0114\n",
      "Epoch 59/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0092 - val_loss: 0.0115\n",
      "Epoch 60/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 61/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0095 - val_loss: 0.0118\n",
      "Epoch 62/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.0099 - val_loss: 0.0113\n",
      "Epoch 63/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0088 - val_loss: 0.0113\n",
      "Epoch 64/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0087 - val_loss: 0.0111\n",
      "Epoch 65/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0085 - val_loss: 0.0112\n",
      "Epoch 66/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0082 - val_loss: 0.0112\n",
      "Epoch 67/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0087 - val_loss: 0.0134\n",
      "Epoch 68/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0088 - val_loss: 0.0115\n",
      "Epoch 69/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 70/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0079 - val_loss: 0.0109\n",
      "Epoch 71/1000\n",
      "57337/57337 [==============================] - 121s - loss: 0.0077 - val_loss: 0.0109\n",
      "Epoch 72/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0076 - val_loss: 0.0111\n",
      "Epoch 73/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 74/1000\n",
      "57337/57337 [==============================] - 118s - loss: 0.0079 - val_loss: 0.0109\n",
      "Epoch 75/1000\n",
      "57337/57337 [==============================] - 119s - loss: 0.0074 - val_loss: 0.0106\n",
      "Epoch 76/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 77/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0070 - val_loss: 0.0102\n",
      "Epoch 78/1000\n",
      "57337/57337 [==============================] - 120s - loss: 0.0070 - val_loss: 0.0108\n",
      "Epoch 79/1000\n",
      "57337/57337 [==============================] - 124s - loss: 0.0079 - val_loss: 0.0131\n",
      "Epoch 80/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0073 - val_loss: 0.0103\n",
      "Epoch 81/1000\n",
      "57337/57337 [==============================] - 122s - loss: 0.0067 - val_loss: 0.0107\n",
      "Epoch 82/1000\n",
      "57337/57337 [==============================] - 128s - loss: 0.0070 - val_loss: 0.0110\n",
      "Epoch 83/1000\n",
      "57337/57337 [==============================] - 129s - loss: 0.0067 - val_loss: 0.0106\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=2048,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=1, callbacks=[earlyStopping_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57337 samples, validate on 16382 samples\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fa179c35f8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_input_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_target_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     verbose=1, callbacks=[earlyStopping_cnn])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=1, callbacks=[earlyStopping_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.fit(train_input_flatten, train_target,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_flatten, valid_target),\n",
    "                    verbose=1, callbacks=[earlyStopping_dnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.plot(history_dnn.history['loss'])\n",
    "plt.plot(history_dnn.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cnn_train', 'cnn_validation',\n",
    "            'lstm_train', 'lstm_validation',\n",
    "            'ffnn_train', 'ffnn_validation'], loc='upper right')\n",
    "#plt.ylim((0.0, 0.02))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dnn.save('models/mBMdnn3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dnn = keras.models.load_model('models/mBMdnnB.h5')\n",
    "model_cnn = keras.models.load_model('models/mBMcnnB.h5')\n",
    "model_lstm = keras.models.load_model('models/mBMlstmB.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyph_predict(word, model,\n",
    "                 length=2, length_after=0, tag_chars='BMES', aslist=False, model_type='dnn'):\n",
    "    \"\"\"Generate tagging from the input word according to the model\"\"\"\n",
    "    word_in = []\n",
    "    word_out = []\n",
    "    generate_network_data([word, len(word)*tag_chars[0]],\n",
    "                          word_in, word_out, length=length,\n",
    "                          length_after=length_after, tag_chars=tag_chars)\n",
    "    word_in = np.reshape(word_in, (len(word_in), (length)*len(hun_chars)))\n",
    "    if model_type=='cnn':\n",
    "        word_in = np.expand_dims(word_in, axis=1) # reshape (x, 1, 259) \n",
    "    word_out = model.predict(word_in)\n",
    "    tag_list = np.array(list(tag_chars))\n",
    "    temp = np.argmax(word_out, axis=1)\n",
    "    temp = tag_list[temp]\n",
    "    if(aslist):\n",
    "        return temp\n",
    "    return \"\".join(temp)\n",
    "\n",
    "\n",
    "def hyph_insterted(word, model,\n",
    "                   length=2, length_after=0, tag_chars='BMES', model_type='dnn'):\n",
    "    tags = hyph_predict(word, model,length,\n",
    "                        length_after, tag_chars, aslist=False, model_type=model_type)\n",
    "    word_inserted = \"\"\n",
    "    if tag_chars=='BM':\n",
    "        for i in range(len(word)):\n",
    "            if i != 0 and tags[i]=='B':\n",
    "                word_inserted += '-'\n",
    "            word_inserted += word[i]\n",
    "    else:\n",
    "        raise NotImplementedError('BM implemented only')\n",
    "    return word_inserted\n",
    "\n",
    "\n",
    "def evaluation(wtags_predicted, wtags_target, tag_chars='BM'): \n",
    "    \"\"\"Compare BMBM with BBMB\"\"\"\n",
    "    if tag_chars!='BM':\n",
    "        raise NotImplementedError(\"Only BM available\")\n",
    "    tp = 0 # target: B prediction: B\n",
    "    tn = 0 # target: M prediction: M\n",
    "    fp = 0 # target: M prediction: B\n",
    "    fn = 0 # target: B prediction: M\n",
    "    for i in range(min(len(wtags_target),len(wtags_predicted))):\n",
    "        c_t = wtags_target[i]\n",
    "        c_p = wtags_predicted[i]\n",
    "        if (c_t == 'B') and (c_p == 'B'):\n",
    "            tp +=1\n",
    "        elif (c_t == 'M') and (c_p == 'M'):\n",
    "            tn +=1\n",
    "        elif (c_t == 'M') and (c_p == 'B'):\n",
    "            fp +=1\n",
    "        elif (c_t == 'B') and (c_p == 'M'):\n",
    "            fn +=1\n",
    "        else:\n",
    "            raise ValueError(\"Not expected tag!\" + c_t + c_p)\n",
    "    good = False\n",
    "    if fn+fp == 0:\n",
    "        good = True\n",
    "    return tp, tn, fp, fn, good\n",
    "\n",
    "\n",
    "def test_ev(model, model_type, model_params = None, num_tests=-1, verbose=1,\n",
    "            hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    \"\"\"Evaulate the tests\"\"\"\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    good = 0\n",
    "    if (verbose>0):\n",
    "        print(\"Prediction\\tTarget\")\n",
    "    if model_type == 'cnn' or model_type =='lstm':\n",
    "        test_result = model.predict(tests_input_cnn[0:num_tests])\n",
    "        for i in range(len(test_result)):\n",
    "            test_tags = result_decode(test_result[i])\n",
    "            test_word = tests_words[i,0]\n",
    "            ev = evaluation(test_tags, tests_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "                \n",
    "\n",
    "    if model_type == 'dnn':\n",
    "        test_range = num_tests\n",
    "        if num_tests==-1:\n",
    "            test_range = len(tests_words)\n",
    "        window_length = model_params[\"window_length\"]\n",
    "        length_after = model_params[\"length_after\"]\n",
    "        tag_chars = model_params[\"tag_chars\"]\n",
    "        for i in range(test_range):\n",
    "            test_word = tests_words[i,0]\n",
    "            test_tags = hyph_predict(test_word, model_dnn,\n",
    "                                       window_length, length_after,\n",
    "                                       tag_chars,\n",
    "                                       model_type=model_type)\n",
    "            ev = evaluation(test_tags, tests_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "    \n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_Fscore = 2 * (test_precision *\n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    ret = {}\n",
    "    ret[\"precision\"] = test_precision\n",
    "    ret[\"recall\"] = test_recall\n",
    "    ret[\"Fscore\"] = test_Fscore\n",
    "    ret[\"word_rate\"] = good/len(tests_words)\n",
    "    return ret\n",
    "        \n",
    "    \n",
    "def result_decode(result, tag_chars='BM'):\n",
    "    \"\"\"[[0,1][0,1]] -> 'MM'\"\"\"\n",
    "    tags = \"\"\n",
    "    result = hardmax(result)\n",
    "    for c in result:\n",
    "        tags += one_hot_decode(c, tag_chars)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def hardmax(arr, axis = -1):\n",
    "    \"\"\"Return 1 if the value is the max in the row, 0 otherwise\n",
    "    [0.2,0.4,0.5]->[0,0,1]\"\"\"\n",
    "    temp = arr - np.max(arr, axis=axis, keepdims=True)\n",
    "    return np.round(1+temp)\n",
    "\n",
    "\n",
    "def hyp_inserted(word, tags, tag_chars='BM'):\n",
    "    \"\"\"insert hyphen to the tags\"\"\"\n",
    "    assert len(word)<=len(tags)\n",
    "    s = \"\"\n",
    "    for c in range(len(word)):\n",
    "        if (c!=0) and tags[c]=='B':\n",
    "            s+='-'\n",
    "        s+=word[c]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 8191 words.\n",
      "CNN: {'Fscore': 0.9699501058361052, 'precision': 0.9552188802858844, 'recall': 0.9851428132678133, 'word_rate': 0.8719326089610548}\n",
      "LSTM: {'Fscore': 0.9849909411652522, 'precision': 0.9786637359305719, 'recall': 0.9914004914004914, 'word_rate': 0.9346844097179832}\n",
      "FFNN: {'Fscore': 0.9879041548268181, 'precision': 0.9882077283552277, 'recall': 0.9876007677543186, 'word_rate': 0.9422536930777683}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Prediction \\t Target\")\n",
    "\n",
    "print(\"Evaluation on\",len(tests_words),\"words.\")\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"window_length\"] = window_length\n",
    "model_params[\"length_after\"] = length_after\n",
    "model_params[\"tag_chars\"] = tag_chars\n",
    "\n",
    "print(\"CNN:\",test_ev(model_cnn,\"cnn\", verbose=0))\n",
    "print(\"LSTM:\",test_ev(model_lstm,\"lstm\", verbose=0))\n",
    "print(\"FFNN:\",test_ev(model_dnn,\"dnn\",model_params=model_params, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69745, 2) (69745, 2)\n",
      "25709 281 43464 291\n"
     ]
    }
   ],
   "source": [
    "model_type = \"dnn\"\n",
    "\n",
    "test_tp = 0\n",
    "test_fp = 0\n",
    "test_tn = 0\n",
    "test_fn = 0\n",
    "test_str = \"\"\n",
    "\n",
    "if model_type == \"dnn\":\n",
    "    test_results = model_dnn.predict(tests_input_flatten)\n",
    "    history = history_dnn\n",
    "else:\n",
    "    if model_type == \"cnn\":\n",
    "        test_results = model_cnn.predict(tests_input_flatten_p1)\n",
    "        history = history_cnn\n",
    "if(np.shape(test_results)[1] == 2):\n",
    "    for i in range(len(test_results)):\n",
    "        # positive\n",
    "        if np.argmax(test_results[i]) == 1:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_tn += 1\n",
    "            else:\n",
    "                test_fn += 1\n",
    "        else:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_fp += 1\n",
    "            else:\n",
    "                test_tp += 1\n",
    "    print(np.shape(test_results),np.shape(tests_target))\n",
    "    print(test_tp,test_fp,test_tn,test_fn)\n",
    "    test_precision = test_tp / (test_tp + test_fp)\n",
    "    test_recall = test_tp / (test_tp + test_fn)\n",
    "    test_Fscore = 2 * (test_precision * \n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    test_str = str(test_precision) + '\\t' + str(test_recall) \n",
    "    test_str += '\\t' + str(test_Fscore)\n",
    "else:\n",
    "    for i in range(len(test_results)):\n",
    "        if np.argmax(test_results[i]) == np.argmax(tests_target[i]):\n",
    "            test_success += 1\n",
    "        else:\n",
    "            test_fail += 1\n",
    "    test_str = str(test_fail/(test_fail+test_success))\n",
    "\n",
    "with open(\"results_data_types.txt\", \"a\") as myfile:\n",
    "    result = \"\"\n",
    "#    result += str(window_length) + '\\t'\n",
    "#    result += str(length_after) + '\\t' + tag_chars\n",
    "#    result += '\\t' + str(num_layers) + '\\t'\n",
    "#    result += str(num_hidden) + '\\t'\n",
    "    result += model_type + '\\t'\n",
    "    result += str(history.epoch[-1]) + '\\t'\n",
    "    result += str(history.history['val_loss'][-1])\n",
    "    result += '\\t' + test_str\n",
    "    result += '\\n'\n",
    "    myfile.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hypher = pyphen.Pyphen(lang='hu_HU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'szemüveg'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model, window_length, length_after, tag_chars, model_type='cnn'),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))\n",
    "\n",
    "test = 'leopárd'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model, window_length, length_after, tag_chars, model_type='cnn'),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = counter_hu_data.most_common()[-400:]\n",
    "print('Prediction\\tTarget')\n",
    "for word in test_words:\n",
    "    next_word = word[0]\n",
    "    if(len(next_word) != 0 and same_char_num(next_word)):\n",
    "        try:\n",
    "            predicted_value = hyph_predict(next_word, model,\n",
    "                                           window_length, length_after,\n",
    "                                           tag_chars, model_type='cnn')\n",
    "            predicted_visual = hyph_insterted(next_word, model,\n",
    "                                              window_length, length_after,\n",
    "                                              tag_chars, model_type='cnn')\n",
    "            excepted_value = hyph_tags_4to2(hyph_tags(next_word))\n",
    "            success = predicted_value == excepted_value\n",
    "            if not success:\n",
    "                print(predicted_visual,\n",
    "                        '\\t',hypher.inserted(next_word))\n",
    "        except ValueError as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4926\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_words)):\n",
    "    if train_words[i,0] == 'topikok':\n",
    "        print(i)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN\tCNN\tLSTM\tTarget\n",
      "a-c-k-n-o-w-l-e-d-g-e\ta-c-k-n-o-w-l-e-d-g-e\ta-c-k-n-o-w-l-e-d-g-e\tac-knowl-edge\n",
      "p-l-a-g-i-a-r-i-z-i-n-g\tp-l-a-g-i-a-r-i-z-i-n-g\tp-l-a-g-i-a-r-i-z-i-n-g\tpla-gia-riz-ing\n",
      "a-r-n-e\ta-r-n-e\ta-r-n-e\tarne\n",
      "c-u-r-f-e-w\tc-u-r-f-e-w\tc-u-r-f-e-w\tcur-few\n",
      "a-n-o-d-e\ta-n-o-d-e\ta-n-o-d-e\tan-ode\n",
      "t-i-l-t-o-n\tt-i-l-t-o-n\tt-i-l-t-o-n\ttilton\n",
      "v-o-y-a-g-e\tv-o-y-a-g-e\tv-o-y-a-g-e\tvoy-age\n",
      "u-n-r-e-c-o-v-e-r-a-b-l-e\tu-n-r-e-c-o-v-e-r-a-b-l-e\tu-n-r-e-c-o-v-e-r-a-b-l-e\tun-re-cov-er-able\n",
      "r-e-s-i-d-e-n-t-i-a-l\tr-e-s-i-d-e-n-t-i-a-l\tr-e-s-i-d-e-n-t-i-a-l\tres-i-den-tial\n",
      "d-o-r-m-a-n-c-y\td-o-r-m-a-n-c-y\td-o-r-m-a-n-c-y\tdor-man-cy\n",
      "a-n-t-i-d-u-m-p-i-n-g\ta-n-t-i-d-u-m-p-i-n-g\ta-n-t-i-d-u-m-p-i-n-g\tan-tidump-ing\n",
      "d-r-i-p\td-r-i-p\td-r-i-p\tdrip\n",
      "b-e-r-h-a-d\tb-e-r-h-a-d\tb-e-r-h-a-d\tberhad\n",
      "c-r-u-m-m-y\tc-r-u-m-m-y\tc-r-u-m-m-y\tcrum-my\n",
      "f-i-s-h-e-r-i-e-s\tf-i-s-h-e-r-i-e-s\tf-i-s-h-e-r-i-e-s\tfish-eries\n",
      "p-t-o-l-e-m-a-i-c\tp-t-o-l-e-m-a-i-c\tp-t-o-l-e-m-a-i-c\tptole-ma-ic\n",
      "d-t-a\td-t-a\td-t-a\tdta\n",
      "k-a-g-o-s-h-i-m-a\tk-a-g-o-s-h-i-m-a\tk-a-g-o-s-h-i-m-a\tkagoshi-ma\n",
      "p-a-r-a-t-h-i-o-n\tp-a-r-a-t-h-i-o-n\tp-a-r-a-t-h-i-o-n\tparathion\n",
      "e-x-t-r-a-t-e-r-r-e-s-t-r-i-a-l-s\te-x-t-r-a-t-e-r-r-e-s-t-r-i-a-l-s\te-x-t-r-a-t-e-r-r-e-s-t-r-i-a-l-s\tex-trater-res-tri-als\n",
      "f-i-d-d-l-e-s\tf-i-d-d-l-e-s\tf-i-d-d-l-e-s\tfid-dles\n",
      "a-d-e-n-o-s-i-n-e\ta-d-e-n-o-s-i-n-e\ta-d-e-n-o-s-i-n-e\tadeno-sine\n",
      "r-e-m-u-n-e-r-a-t-i-v-e\tr-e-m-u-n-e-r-a-t-i-v-e\tr-e-m-u-n-e-r-a-t-i-v-e\tre-mu-ner-a-tive\n",
      "c-a-l-e-n-d-a-r-i-n-g\tc-a-l-e-n-d-a-r-i-n-g\tc-a-l-e-n-d-a-r-i-n-g\tcal-en-dar-ing\n",
      "g-e-n-t-l-e-s-t\tg-e-n-t-l-e-s-t\tg-e-n-t-l-e-s-t\tgen-tlest\n",
      "a-r-t-i-s-t-r-y\ta-r-t-i-s-t-r-y\ta-r-t-i-s-t-r-y\tartistry\n",
      "h-o-o-p-a\th-o-o-p-a\th-o-o-p-a\thoopa\n",
      "a-s-p-n-e-t\ta-s-p-n-e-t\ta-s-p-n-e-t\tasp-net\n",
      "p-r-o-n-u-n-c-i-a-t-i-o-n\tp-r-o-n-u-n-c-i-a-t-i-o-n\tp-r-o-n-u-n-c-i-a-t-i-o-n\tpro-nun-ci-a-tion\n",
      "p-k-i\tp-k-i\tp-k-i\tpki\n",
      "n-n-p\tn-n-p\tn-n-p\tnnp\n",
      "s-e-a-p-o-r-t\ts-e-a-p-o-r-t\ts-e-a-p-o-r-t\tsea-port\n",
      "m-i-c-r-o-s-c-o-p-e-s\tm-i-c-r-o-s-c-o-p-e-s\tm-i-c-r-o-s-c-o-p-e-s\tmi-cro-scopes\n",
      "m-o-d-u-l-o\tm-o-d-u-l-o\tm-o-d-u-l-o\tmod-u-lo\n",
      "n-o-n-s-p-e-c-i-a-l-i-s-t\tn-o-n-s-p-e-c-i-a-l-i-s-t\tn-o-n-s-p-e-c-i-a-l-i-s-t\tnon-spe-cial-ist\n",
      "i-n-g-r-e-s\ti-n-g-r-e-s\ti-n-g-r-e-s\tin-gres\n",
      "f-r-e-n-c-h-y\tf-r-e-n-c-h-y\tf-r-e-n-c-h-y\tfrenchy\n",
      "o-r-g-a-s-m-i-c\to-r-g-a-s-m-i-c\to-r-g-a-s-m-i-c\tor-gas-mic\n",
      "w-a-r-r-a-n-t-y\tw-a-r-r-a-n-t-y\tw-a-r-r-a-n-t-y\twar-ran-ty\n",
      "p-h-y-t-o-e-s-t-r-o-g-e-n-s\tp-h-y-t-o-e-s-t-r-o-g-e-n-s\tp-h-y-t-o-e-s-t-r-o-g-e-n-s\tphy-toe-stro-gens\n"
     ]
    }
   ],
   "source": [
    "hypher = pyphen.Pyphen(lang='en_EN')\n",
    "\n",
    "N = 60\n",
    "print(\"FFNN\\tCNN\\tLSTM\\tTarget\")\n",
    "for i in range(len(tests_words[0:40])):\n",
    "    hyphs = \"\"\n",
    "    test_word = tests_words[i,0]\n",
    "    test_ic = train_input_cnn[i:i+1]\n",
    "    test_tags = hyph_predict(test_word, model_dnn,\n",
    "                             window_length, length_after,\n",
    "                             tag_chars,\n",
    "                             model_type=\"dnn\")\n",
    "    ffnn = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=ffnn+'\\t'\n",
    "    \n",
    "    test_result = model_cnn.predict(test_ic)\n",
    "    test_result = test_result[0]\n",
    "    test_tags = result_decode(test_result)\n",
    "    \n",
    "    cnn = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=cnn+'\\t'\n",
    "    \n",
    "    test_result = model_lstm.predict(test_ic)\n",
    "    test_result = test_result[0]\n",
    "    test_tags = result_decode(test_result)\n",
    "    \n",
    "    lstm = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=lstm+'\\t'\n",
    "    \n",
    "    target = hypher.inserted(test_word) \n",
    "    hyphs += target\n",
    "    #if (ffnn!=target) or (cnn!=target) or (lstm!=target):\n",
    "    print(hyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-386060583b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhardmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bc736ad03449>\u001b[0m in \u001b[0;36mone_hot_encode\u001b[0;34m(char, dictionary)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BMES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "one_hot_encode(hardmax(test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61187395,  0.66373782,  0.82366587,  0.89691012,  0.51733389])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vi-deó'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypher.inserted('videó')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
