{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyphen\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv1D, Embedding, LSTM, Input\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import langdetect as ld\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext pep8_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hun_chars = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz' + '^$'  # ^,$\n",
    "\n",
    "\n",
    "def hyph_tags(word, hypher, aslist=False):\n",
    "    \"\"\"Hyphenating classification of the characters in the word.\n",
    "    {B(egin),M(iddle),E(nd),S(ingle)}\"\"\"\n",
    "    if (len(word) == 0):\n",
    "        raise IndexError(\"0 length word\")\n",
    "    ret = list('M' * len(word))\n",
    "    ret[0] = 'B'\n",
    "    ret[-1] = 'E'\n",
    "    for i in hypher.positions(word):\n",
    "        ret[i] = 'B'\n",
    "        if(ret[i-1] == 'B'):\n",
    "            ret[i-1] = 'S'\n",
    "        else:\n",
    "            ret[i-1] = 'E'\n",
    "    if (aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def hyph_tags_4to2(word, aslist=False):\n",
    "    \"\"\"{B,M,E,S} to {B, M}\"\"\"\n",
    "    ret = list(word)\n",
    "    for i in range(len(ret)):\n",
    "        if ret[i] == 'S':\n",
    "            ret[i] = 'B'\n",
    "        if ret[i] != 'B':\n",
    "            ret[i] = 'M'\n",
    "    if(aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def same_char_num(word, hypher):\n",
    "    \"\"\"Return true if the hyphenated word has as many chars as the original\"\"\"\n",
    "    return len(hypher.inserted(word)) == len(word)+len(hypher.positions(word))\n",
    "def only_hyphen_inserted(word, hypher):\n",
    "    \"\"\"Return true if the hyphenation is only hyphen addition\"\"\"\n",
    "    target = hypher.inserted(word)\n",
    "    i=0\n",
    "    j=0\n",
    "    while (i<len(word)) and (j<len(target)):\n",
    "        if word[i]==target[j]:\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif target[j]=='-':\n",
    "            j+=1\n",
    "        else:\n",
    "            return False\n",
    "    if i==len(word) and j==len(target):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cleaning(data):\n",
    "    \"\"\"Text cleaning:\n",
    "        lower the letters\n",
    "        punctuation, digits ellimination\"\"\"\n",
    "    formated_data = data.lower()\n",
    "    formated_data = re.sub('['+string.punctuation+']', '', formated_data)\n",
    "    formated_data = re.sub('['+string.digits+']', '', formated_data)\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# onehot: {'B','M','E','S'}\n",
    "def one_hot_encode(char, dictionary='BM'):\n",
    "    ret = [0]*len(dictionary)\n",
    "    if char in dictionary:\n",
    "        ret[dictionary.find(char)] = 1\n",
    "        return ret\n",
    "    raise ValueError('Value out of dictionary range: '+char)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    \"\"\"Randomize 2 same length array in the same permutation\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def one_hot_decode(arr, dictionary='BM'):\n",
    "    assert len(arr) == len(dictionary)\n",
    "    i = np.nonzero(arr)[0][0]\n",
    "    return dictionary[i]\n",
    "\n",
    "\n",
    "def generate_network_data(data, ret_input=[], ret_output=[],\n",
    "                          length=2, length_after=0,\n",
    "                          start_char='^', end_char='$',\n",
    "                          chars=hun_chars, tag_chars='BMES'):\n",
    "    \"\"\"from [word,hyph_class(word) to length-long input-output data\"\"\"\n",
    "    word = data[0]\n",
    "    word_plus = start_char*(length-length_after-1)+word+end_char*length_after\n",
    "    hyph_word = data[1]\n",
    "    for i in range(0, len(word)):\n",
    "        input_next_iter = []\n",
    "        for c in word_plus[i:i+length]:\n",
    "            input_next_iter.append(one_hot_encode(c, chars))\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return\n",
    "\n",
    "def generate_network_words(data, padding=None, start_char='^',\n",
    "                           end_char='$', chars=hun_chars,\n",
    "                           tag_chars='BM', tag_default=-1):\n",
    "    \"\"\"One-hot [word, hyph_class(word)]->[[[010],[010]],[[01],[01]]]\n",
    "    padding to fixed size, if not null\"\"\"\n",
    "    ret_input=[]\n",
    "    ret_output=[]\n",
    "    \n",
    "    word = data[0]\n",
    "    hyph_word = data[1]\n",
    "    if padding != None:\n",
    "        if len(word)>padding:\n",
    "            raise IndexError(\"The word is longer than the fixed size\")\n",
    "        else:\n",
    "            word = word + (padding-len(word))*end_char\n",
    "            hyph_word = hyph_word + (padding-len(hyph_word)) * tag_chars[tag_default]\n",
    "    for i in range(0,len(word)):\n",
    "        input_next_iter = one_hot_encode(word[i],chars)\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return ret_input, ret_output\n",
    "    \n",
    "def hyph_tupples(data, hypher,\n",
    "                tag_chars='BM', filter_data='same_char'):\n",
    "    \"\"\"[words] -> [words, hyph_words]\n",
    "    filter_data: same_char, only_hyphens, no_filter\"\"\"\n",
    "    word_list = []\n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    for next_word in data:\n",
    "        c_all += 1\n",
    "        good_word = True\n",
    "        if filter_data == 'same_char':\n",
    "            good_word = same_char_num(next_word, hypher)\n",
    "        elif filter_data == 'only_hyphens':\n",
    "            good_word = only_hyphen_inserted(next_word, hypher)\n",
    "        elif filter_data == 'no_filter':\n",
    "            good_word = len(next_word)>0\n",
    "        else:\n",
    "            raise ValueError('filter_data not supported' + filter_data)\n",
    "        if(len(next_word) != 0 and good_word):\n",
    "            c_same_char_num += 1\n",
    "            if(len(tag_chars) == 2):\n",
    "                word_list.append([next_word,\n",
    "                                  hyph_tags_4to2(hyph_tags(next_word, hypher=hypher))])\n",
    "            else:\n",
    "                word_list.append([next_word, hyph_tags(next_word, hypher=hypher)])\n",
    "    return word_list, c_all, c_same_char_num\n",
    "\n",
    "def tupple_to_train(word_list, window_length, length_after,\n",
    "                 tag_chars='BM'):\n",
    "    \"\"\"[words, hyph_words] -> in[0,1,0...], out[0,1,0...]\"\"\"\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            generate_network_data(word, data_in, data_out,\n",
    "                                  window_length, tag_chars=tag_chars,\n",
    "                                  length_after=length_after)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "    return data_in, data_out, wrong_word\n",
    "\n",
    "def bigram_counter_from_file(filename):\n",
    "    \"\"\"creates bigram counter from file\"\"\"\n",
    "    with open(filename) as f:\n",
    "        word_list = []\n",
    "        for words in f:\n",
    "            words = words.strip()\n",
    "            words = words.split()\n",
    "            for w in words:\n",
    "                w = cleaning(w)\n",
    "                if len(w)>0:\n",
    "                    word_list.append(w)\n",
    "\n",
    "    bigram_counter = collections.Counter()\n",
    "    for word in word_list:\n",
    "        for i in range(2,len(word)):\n",
    "            bigram_counter[word[i-2:i]] += 1\n",
    "    return bigram_counter\n",
    "\n",
    "def bigrams_in_word(word, bigram_counter, mc=100):\n",
    "    bigrams = np.array(bigram_counter.most_common(mc))[:,0]\n",
    "    w_bc = len(word)-1\n",
    "    if w_bc<1:\n",
    "        return 1.0\n",
    "    w_bf = 0\n",
    "    for i in range(2,len(word)):\n",
    "        if word[i-2:i] in bigrams:\n",
    "            w_bf +=1\n",
    "    return w_bf/w_bc\n",
    "\n",
    "def bigram_selector(word, bigram_counters,threshold=0.2, mc=100):\n",
    "    \"\"\"Choose the language of the word\"\"\"\n",
    "    lang_likes = np.zeros(len(bigram_counters)+1)\n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        lang_likes[i] = bigrams_in_word(word, bigram_counters[i], mc)\n",
    "    lang_likes_max = np.argmax(lang_likes)\n",
    "    \n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        if i!=lang_likes_max:\n",
    "            if lang_likes[lang_likes_max]-lang_likes[i]<=threshold:\n",
    "                return len(bigram_counters)\n",
    "    return lang_likes_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_reader(file, tail_cut=100000,\n",
    "                lang_selector = False, lang_thr=0.6,\n",
    "                lang_file_en='../wikipedia/angol/ossz_angol',\n",
    "                lang_file_hu='../wikipedia/magyar/ossz_magyar'):\n",
    "    \"\"\"Read data from file\"\"\"\n",
    "\n",
    "    if lang_selector:\n",
    "        bigram_counter_en = bigram_counter_from_file(lang_file_en)\n",
    "        bigram_counter_hu = bigram_counter_from_file(lang_file_hu)\n",
    "        out_en_words = 0\n",
    "    \n",
    "    tail_cut_ptest_words = tail_cut + 500\n",
    "\n",
    "    counter_hu_data = collections.Counter()\n",
    "    with open(file, 'r',\n",
    "              errors='ignore', encoding='latin2') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i = i+1\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if(words[1].isdigit()):\n",
    "                    cword = cleaning(words[0])\n",
    "                    if lang_selector:\n",
    "                        lang = bigram_selector(cword,\n",
    "                                            [bigram_counter_hu,\n",
    "                                             bigram_counter_en],\n",
    "                                            lang_thr)\n",
    "                        if (lang!=1):\n",
    "                            counter_hu_data[cword] += int(words[1])\n",
    "                        else:\n",
    "                            out_en_words +=1\n",
    "                    else:\n",
    "                        counter_hu_data[cword] += int(words[1])\n",
    "            if i > tail_cut_ptest_words:\n",
    "                break\n",
    "    if lang_selector:\n",
    "        print(\"Throwed english words: \", out_en_words)\n",
    "    return counter_hu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_generator(data_counter, window_length, length_after,\n",
    "                         tag_chars='BM', tail_cut=100000,\n",
    "                         valid_rate=0.2, test_rate=0.1,\n",
    "                         language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "    unique words -> characters -> randomize -> cut\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                    hypher=hypher)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "\n",
    "\n",
    "def train_data_generator_uwords(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1,\n",
    "                                language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> randomize -> cut -> characters\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    np.random.shuffle(data_list)\n",
    "    data_len = len(data_list)\n",
    "    tests_data = data_list[0:int(data_len*test_rate)]\n",
    "    valid_data = data_list[int(data_len*test_rate):\n",
    "                           int(data_len*(test_rate+valid_rate))]\n",
    "    train_data = data_list[int(data_len*(test_rate+valid_rate)):]\n",
    "    \n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    tests_list, c_all_p, c_same_char_num_p = hyph_tupples(tests_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    valid_list, c_all_p, c_same_char_num_p = hyph_tupples(valid_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    train_list, c_all_p, c_same_char_num_p = hyph_tupples(train_data,\n",
    "                                                          tag_chars=tag_chars,\n",
    "                                                          hypher=hypher)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    \n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    wrong_word = 0\n",
    "    tests_input, tests_target, wrong_w_p = tupple_to_train(tests_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    valid_input, valid_target, wrong_w_p = tupple_to_train(valid_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    train_input, train_target, wrong_w_p = tupple_to_train(train_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    \n",
    "\n",
    "def train_data_generator_uchars(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1,\n",
    "                                language='hu'):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> characters -> unique -> randomize -> cut\"\"\"\n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                     hypher=hypher)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    #Unique\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    \n",
    "    shape_in = np.shape(data_in)\n",
    "    shape_out = np.shape(data_out)\n",
    "    \n",
    "    data_in_flatten = np.reshape(\n",
    "        data_in, (shape_in[0], shape_in[1]*shape_in[2]))\n",
    "    shape_in_flatten = np.shape(data_in_flatten)\n",
    "    \n",
    "    data_iosum = np.concatenate((data_in_flatten, data_out), axis=1)\n",
    "    data_iosum_unique = np.vstack({tuple(row) for row in data_iosum})\n",
    "    \n",
    "    data_in = data_iosum_unique[:,:-shape_out[1]]\n",
    "    data_out = data_iosum_unique[:,-shape_out[1]:]\n",
    "    print('Data unique len: ', np.shape(data_iosum_unique)[0])\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_words(data_counter, tag_chars='BM', padding = 30, tail_cut=100000,\n",
    "                     valid_rate=0.2, test_rate=0.1,\n",
    "                     language='hu', no_split=False,\n",
    "                     filter_data = 'same_char'):\n",
    "    \"\"\"Training data, example: alma -> {[[1,0..][0,0..][0,0...][0,0...]],[[1,0],[0,1][1,0][0,1]]}\"\"\"\n",
    "    \n",
    "    if language == 'hu':\n",
    "        hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "        print(\"alma\")\n",
    "    elif language == 'en':\n",
    "        hypher = pyphen.Pyphen(lang='en_EN')\n",
    "        print(\"apple\")\n",
    "        \n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars,\n",
    "                                                    hypher=hypher,\n",
    "                                                    filter_data=filter_data)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    data_words = []\n",
    "    wrong_word = 0\n",
    "    long_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            next_data_in, next_data_out = generate_network_words(word, padding = padding, tag_chars=tag_chars)\n",
    "            next_data_in = np.array(next_data_in, dtype='float32')\n",
    "            next_data_out = np.array(next_data_out, dtype='float32')\n",
    "            data_in.append(next_data_in)\n",
    "            data_out.append(next_data_out)\n",
    "            data_words.append(word)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "        except IndexError:\n",
    "            long_word += 1\n",
    "            print(word)\n",
    "            \n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "    print('Words longer than the padding: ', long_word)\n",
    "    \n",
    "    data_in = np.array(data_in)\n",
    "    data_out = np.array(data_out)\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    order = np.random.permutation(data_len)\n",
    "    data_in = [data_in[k] for k in order]\n",
    "    data_out = [data_out[k] for k in order]\n",
    "    data_words = [data_words[k] for k in order]\n",
    "    \n",
    "    #data_in, data_out, word_list = unison_shuffled_copies(data_in, data_out, word_list)\n",
    "    \n",
    "    datas = {}\n",
    "    \n",
    "    if no_split:\n",
    "        datas[\"input\"] = np.array(data_in)\n",
    "        datas[\"target\"] = np.array(data_out)\n",
    "        datas[\"words\"] = np.array(data_words)\n",
    "        return datas, wrong_word, long_word\n",
    "    \n",
    "    datas[\"tests_words\"] = np.array(data_words[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_input\"] = np.array(data_in[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_target\"] = np.array(data_out[0:int(data_len*test_rate)])\n",
    "    datas[\"valid_words\"] = np.array(data_words[int(data_len*test_rate):\n",
    "                                               int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_input\"] = np.array(data_in[int(data_len*test_rate):\n",
    "                                            int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_target\"] = np.array(data_out[int(data_len*test_rate):\n",
    "                                              int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"train_words\"] = np.array(data_words[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_input\"] = np.array(data_in[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_target\"] = np.array(data_out[int(data_len*(test_rate+valid_rate)):])\n",
    "    \n",
    "    return datas, wrong_word, long_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_creator_dnn(window_length, output_length, num_layers=1,\n",
    "                  num_hidden=10, chars=hun_chars):\n",
    "    \"\"\"Creates Keras model with the given input, output dimensions\n",
    "    and layer number, hidden layer length\"\"\"\n",
    "    \n",
    "    input_shape = window_length*len(chars)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(input_dim=(input_shape),\n",
    "                    units=num_hidden, name='input_layer',\n",
    "                    activation='sigmoid'))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(units=num_hidden, activation='sigmoid'))\n",
    "\n",
    "    # model.add(Flatten())\n",
    "    model.add(Dense(output_length, name='output_layer', activation='softmax'))\n",
    "\n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_cnn(output_length,\n",
    "                      num_layers=1, num_hidden=516,\n",
    "                      kernel_size=10, strides=1, word_length = 30, chars=hun_chars):\n",
    "    \"\"\"Creates Keras CNN model\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(num_hidden,kernel_size, strides=strides, padding=\"same\",\n",
    "                     activation='relu', input_shape=(word_length, len(chars))))\n",
    "    for i in range(1,num_layers):\n",
    "        model.add(Conv1D(num_hidden,kernel_size,strides=strides,\n",
    "                         padding=\"same\", activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_lstm(output_length, \n",
    "                       num_layers=2, num_hidden=64,\n",
    "                       word_length = 30, chars = hun_chars):\n",
    "    \"\"\"Creates Keras LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(num_hidden, activation='relu',return_sequences=True,\n",
    "                   go_backwards=True,\n",
    "                   input_shape=(word_length, len(chars))))\n",
    "    for i in range(1,num_layers):\n",
    "        model.add(LSTM(num_hidden, activation='relu',return_sequences=True,\n",
    "                       go_backwards=True,))\n",
    "    \n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padding = 30\n",
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 3\n",
    "num_hidden = 150\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "\n",
    "datas, wrong_words, long_word = train_data_words(counter_hu_data, tag_chars,\n",
    "                                                 padding, tail_cut, language='hu')\n",
    "\n",
    "tests_input_cnn = datas[\"tests_input\"]\n",
    "tests_target_cnn = datas[\"tests_target\"]\n",
    "valid_input_cnn = datas[\"valid_input\"]\n",
    "valid_target_cnn = datas[\"valid_target\"]\n",
    "train_input_cnn = datas[\"train_input\"]\n",
    "train_target_cnn = datas[\"train_target\"]\n",
    "\n",
    "tests_words = datas[\"tests_words\"]\n",
    "valid_words = datas[\"valid_words\"]\n",
    "train_words = datas[\"train_words\"]\n",
    "\n",
    "\n",
    "print(np.shape(train_input_cnn), np.shape(valid_input_cnn), np.shape(tests_input_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_word = 0\n",
    "tests_input, tests_target, wrong_w_p = tupple_to_train(tests_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "valid_input, valid_target, wrong_w_p = tupple_to_train(valid_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "train_input, train_target, wrong_w_p = tupple_to_train(train_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "\n",
    "print(\"Still wrong word (expected zero): \", wrong_word)\n",
    "\n",
    "train_input_flatten = np.reshape(\n",
    "    train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "valid_input_flatten = np.reshape(\n",
    "    valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "tests_input_flatten = np.reshape(\n",
    "    tests_input, (len(tests_input), (window_length)*len(hun_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 5\n",
    "num_hidden = 110\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "[train_input_flatten, train_target,\n",
    " valid_input_flatten, valid_target,\n",
    " tests_input_flatten,\n",
    " tests_target] = train_data_generator(counter_hu_data,\n",
    "                                             window_length,\n",
    "                                             length_after,\n",
    "                                             tag_chars,\n",
    "                                             tail_cut)\n",
    "\n",
    "#train_input_flatten_p1 = np.expand_dims(train_input_flatten, axis=1) # reshape (X, 1, 259) \n",
    "#valid_input_flatten_p1 = np.expand_dims(valid_input_flatten, axis=1)\n",
    "#tests_input_flatten_p1 = np.expand_dims(tests_input_flatten, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the keras model\n",
    "model_dnn = model_creator_dnn(window_length, len(tag_chars),\n",
    "                      num_layers, num_hidden)\n",
    "\n",
    "earlyStopping_dnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "earlyStopping_cnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "model_lstm = model_creator_lstm(len(tag_chars), num_layers=2, num_hidden=128)\n",
    "\n",
    "earlyStopping_lstm = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "print('Models created. Start training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_lstm, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=2048,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=1, callbacks=[earlyStopping_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_cnn = model_cnn.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=1, callbacks=[earlyStopping_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.fit(train_input_flatten, train_target,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_flatten, valid_target),\n",
    "                    verbose=1, callbacks=[earlyStopping_dnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.plot(history_dnn.history['loss'])\n",
    "plt.plot(history_dnn.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cnn_train', 'cnn_validation',\n",
    "            'lstm_train', 'lstm_validation',\n",
    "            'ffnn_train', 'ffnn_validation'], loc='upper right')\n",
    "#plt.ylim((0.0, 0.02))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dnn.save('models/mBMdnn3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dnn = keras.models.load_model('models/mBMdnnB.h5')\n",
    "model_cnn = keras.models.load_model('models/mBMcnnB.h5')\n",
    "model_lstm = keras.models.load_model('models/mBMlstmB.h5')\n",
    "model_s2s = keras.models.load_model('models/s2s71024.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyph_predict(word, model, model_type,\n",
    "                 length=2, length_after=0, tag_chars='BMES', aslist=False, padding = 30):\n",
    "    \"\"\"Generate tagging from the input word according to the model\"\"\"\n",
    "    word_in = []\n",
    "    word_out = []\n",
    "    if model_type == 'dnn':\n",
    "        generate_network_data([word, len(word)*tag_chars[0]],\n",
    "                              word_in, word_out, length=length,\n",
    "                              length_after=length_after, tag_chars=tag_chars)\n",
    "        word_in = np.reshape(word_in, (len(word_in), (length)*len(hun_chars)))\n",
    "        word_out = model.predict(word_in)\n",
    "        tag_list = np.array(list(tag_chars))\n",
    "        temp = np.argmax(word_out, axis=1)\n",
    "        temp = tag_list[temp]\n",
    "        if(aslist):\n",
    "            return temp\n",
    "        return \"\".join(temp)\n",
    "    if model_type=='cnn' or model_type=='lstm':\n",
    "        data = create_input_from_words([[word,'']],padding = padding)\n",
    "        data_input_cnn = data['input_cnn']\n",
    "        data_input_cnn\n",
    "        word_out = model.predict(data_input_cnn)\n",
    "        tags = []\n",
    "        for i in word_out:\n",
    "            tags.append(result_decode(i))\n",
    "        return tags[0]\n",
    "\n",
    "\n",
    "def hyph_insterted(word, model, model_type,\n",
    "                   length=2, length_after=0, tag_chars='BMES', padding = 30):\n",
    "    tags = hyph_predict(word, model, model_type, length,\n",
    "                        length_after, tag_chars, aslist=False, padding=padding)\n",
    "    word_inserted = \"\"\n",
    "    if tag_chars=='BM':\n",
    "        for i in range(len(word)):\n",
    "            if i != 0 and tags[i]=='B':\n",
    "                word_inserted += '-'\n",
    "            word_inserted += word[i]\n",
    "    else:\n",
    "        raise NotImplementedError('BM implemented only')\n",
    "    return word_inserted\n",
    "\n",
    "\n",
    "def evaluation(wtags_predicted, wtags_target, tag_chars='BM'): \n",
    "    \"\"\"Compare BMBM with BBMB\"\"\"\n",
    "    if tag_chars!='BM':\n",
    "        raise NotImplementedError(\"Only BM available\")\n",
    "    tp = 0 # target: B prediction: B\n",
    "    tn = 0 # target: M prediction: M\n",
    "    fp = 0 # target: M prediction: B\n",
    "    fn = 0 # target: B prediction: M\n",
    "    for i in range(min(len(wtags_target),len(wtags_predicted))):\n",
    "        c_t = wtags_target[i]\n",
    "        c_p = wtags_predicted[i]\n",
    "        if (c_t == 'B') and (c_p == 'B'):\n",
    "            tp +=1\n",
    "        elif (c_t == 'M') and (c_p == 'M'):\n",
    "            tn +=1\n",
    "        elif (c_t == 'M') and (c_p == 'B'):\n",
    "            fp +=1\n",
    "        elif (c_t == 'B') and (c_p == 'M'):\n",
    "            fn +=1\n",
    "        else:\n",
    "            raise ValueError(\"Not expected tag!\" + c_t + c_p)\n",
    "    good = False\n",
    "    if fn+fp == 0:\n",
    "        good = True\n",
    "    return tp, tn, fp, fn, good\n",
    "\n",
    "\n",
    "def test_ev(model, model_type,\n",
    "            hypher, model_params = None, num_tests=-1, verbose=1, tests_data = None):\n",
    "    \"\"\"Evaulate the tests\"\"\"\n",
    "    if tests_data:\n",
    "        test_input_cnn = tests_data[\"input_cnn\"]\n",
    "        test_words = tests_data[\"words\"]\n",
    "    else:\n",
    "        test_input_cnn = tests_input_cnn\n",
    "        test_words = tests_words\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    good = 0\n",
    "    if (verbose>0):\n",
    "        print(\"Prediction\\tTarget\")\n",
    "    if model_type == 'cnn' or model_type =='lstm':\n",
    "        test_result = model.predict(test_input_cnn[0:num_tests])\n",
    "        for i in range(len(test_result)):\n",
    "            test_tags = result_decode(test_result[i])\n",
    "            test_word = test_words[i,0]\n",
    "            ev = evaluation(test_tags, test_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "                \n",
    "\n",
    "    if model_type == 'dnn':\n",
    "        test_range = num_tests\n",
    "        if num_tests==-1:\n",
    "            test_range = len(test_words)\n",
    "        window_length = model_params[\"window_length\"]\n",
    "        length_after = model_params[\"length_after\"]\n",
    "        tag_chars = model_params[\"tag_chars\"]\n",
    "        for i in range(test_range):\n",
    "            test_word = test_words[i,0]\n",
    "            test_tags = hyph_predict(test_word, model_dnn, model_type,\n",
    "                                       window_length, length_after,\n",
    "                                       tag_chars)\n",
    "            ev = evaluation(test_tags, test_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "    \n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_Fscore = 2 * (test_precision *\n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    ret = {}\n",
    "    ret[\"precision\"] = test_precision\n",
    "    ret[\"recall\"] = test_recall\n",
    "    ret[\"Fscore\"] = test_Fscore\n",
    "    ret[\"word_rate\"] = good/len(test_words)\n",
    "    return ret\n",
    "        \n",
    "    \n",
    "def result_decode(result, tag_chars='BM'):\n",
    "    \"\"\"[[0,1][0,1]] -> 'MM'\"\"\"\n",
    "    tags = \"\"\n",
    "    result = hardmax(result)\n",
    "    for c in result:\n",
    "        tags += one_hot_decode(c, tag_chars)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def hardmax(arr, axis = -1):\n",
    "    \"\"\"Return 1 if the value is the max in the row, 0 otherwise\n",
    "    [0.2,0.4,0.5]->[0,0,1]\"\"\"\n",
    "    temp = arr - np.max(arr, axis=axis, keepdims=True)\n",
    "    return np.round(1+temp)\n",
    "\n",
    "\n",
    "def hyp_inserted(word, tags, tag_chars='BM'):\n",
    "    \"\"\"insert hyphen to the tags\"\"\"\n",
    "    assert len(word)<=len(tags)\n",
    "    s = \"\"\n",
    "    for c in range(len(word)):\n",
    "        if (c!=0) and tags[c]=='B':\n",
    "            s+='-'\n",
    "        s+=word[c]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Prediction \\t Target\")\n",
    "hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "print(\"Evaluation on\",len(tests_words),\"words.\")\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"window_length\"] = window_length\n",
    "model_params[\"length_after\"] = length_after\n",
    "model_params[\"tag_chars\"] = tag_chars\n",
    "\n",
    "print(\"CNN:\",test_ev(model_cnn,\"cnn\",hypher, verbose=0))\n",
    "print(\"LSTM:\",test_ev(model_lstm,\"lstm\",hypher, verbose=0))\n",
    "print(\"FFNN:\",test_ev(model_dnn,\"dnn\",hypher, model_params=model_params, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_type = \"dnn\"\n",
    "\n",
    "test_tp = 0\n",
    "test_fp = 0\n",
    "test_tn = 0\n",
    "test_fn = 0\n",
    "test_str = \"\"\n",
    "\n",
    "if model_type == \"dnn\":\n",
    "    test_results = model_dnn.predict(tests_input_flatten)\n",
    "    history = history_dnn\n",
    "else:\n",
    "    if model_type == \"cnn\":\n",
    "        test_results = model_cnn.predict(tests_input_flatten_p1)\n",
    "        history = history_cnn\n",
    "if(np.shape(test_results)[1] == 2):\n",
    "    for i in range(len(test_results)):\n",
    "        # positive\n",
    "        if np.argmax(test_results[i]) == 1:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_tn += 1\n",
    "            else:\n",
    "                test_fn += 1\n",
    "        else:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_fp += 1\n",
    "            else:\n",
    "                test_tp += 1\n",
    "    print(np.shape(test_results),np.shape(tests_target))\n",
    "    print(test_tp,test_fp,test_tn,test_fn)\n",
    "    test_precision = test_tp / (test_tp + test_fp)\n",
    "    test_recall = test_tp / (test_tp + test_fn)\n",
    "    test_Fscore = 2 * (test_precision * \n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    test_str = str(test_precision) + '\\t' + str(test_recall) \n",
    "    test_str += '\\t' + str(test_Fscore)\n",
    "else:\n",
    "    for i in range(len(test_results)):\n",
    "        if np.argmax(test_results[i]) == np.argmax(tests_target[i]):\n",
    "            test_success += 1\n",
    "        else:\n",
    "            test_fail += 1\n",
    "    test_str = str(test_fail/(test_fail+test_success))\n",
    "\n",
    "with open(\"results_data_types.txt\", \"a\") as myfile:\n",
    "    result = \"\"\n",
    "#    result += str(window_length) + '\\t'\n",
    "#    result += str(length_after) + '\\t' + tag_chars\n",
    "#    result += '\\t' + str(num_layers) + '\\t'\n",
    "#    result += str(num_hidden) + '\\t'\n",
    "    result += model_type + '\\t'\n",
    "    result += str(history.epoch[-1]) + '\\t'\n",
    "    result += str(history.history['val_loss'][-1])\n",
    "    result += '\\t' + test_str\n",
    "    result += '\\n'\n",
    "    myfile.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hypher = pyphen.Pyphen(lang='hu_HU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'szemüveg'\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model_cnn, 'cnn', window_length, length_after, tag_chars),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))\n",
    "\n",
    "test = 'leopárd'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model_cnn,'cnn', window_length, length_after, tag_chars),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = counter_hu_data.most_common()[-400:]\n",
    "print('Prediction\\tTarget')\n",
    "for word in test_words:\n",
    "    next_word = word[0]\n",
    "    if(len(next_word) != 0 and same_char_num(next_word)):\n",
    "        try:\n",
    "            predicted_value = hyph_predict(next_word, model,'cnn',\n",
    "                                           window_length, length_after,\n",
    "                                           tag_chars)\n",
    "            predicted_visual = hyph_insterted(next_word, model,'cnn',\n",
    "                                              window_length, length_after,\n",
    "                                              tag_chars)\n",
    "            excepted_value = hyph_tags_4to2(hyph_tags(next_word))\n",
    "            success = predicted_value == excepted_value\n",
    "            if not success:\n",
    "                print(predicted_visual,\n",
    "                        '\\t',hypher.inserted(next_word))\n",
    "        except ValueError as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_words)):\n",
    "    if train_words[i,0] == 'topikok':\n",
    "        print(i)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HU-EN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests_hu_words = read_hu_en_data(\"hu_en_datas/hu_tests.txt\")\n",
    "tests_hu_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_hu_en_data(file):\n",
    "    data_words = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            word = line.split()\n",
    "            data_words.append(word)\n",
    "    return data_words\n",
    "\n",
    "def create_input_from_words(words, padding=30, dnn=False, tag_chars='BM'):\n",
    "    data = {}\n",
    "    data[\"words\"] = np.array(words)\n",
    "    data_input_cnn = []\n",
    "    data_target_cnn = []\n",
    "    for word in words:\n",
    "        next_data_in, next_data_out = generate_network_words(word, padding = padding, tag_chars=tag_chars)\n",
    "        data_input_cnn.append(np.array(next_data_in, dtype='float32'))\n",
    "        data_target_cnn.append(np.array(next_data_out, dtype='float32'))\n",
    "    data[\"input_cnn\"] = np.array(data_input_cnn)\n",
    "    data[\"target_cnn\"] = np.array(data_target_cnn)\n",
    "    if dnn:\n",
    "        data_input, data_target, _ = tupple_to_train(words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "\n",
    "        data_input_flatten = np.reshape(\n",
    "            data_input, (len(data_input), (window_length)*len(hun_chars)))\n",
    "        data[\"target\"] = np.array(data_target)\n",
    "        data[\"input_flatten\"] = np.array(data_input_flatten)\n",
    "    return data\n",
    "\n",
    "def shuffle_concat_2D(arr1,arr2):\n",
    "    ret = np.append(arr1,arr2,axis=0)\n",
    "    order = np.random.permutation(len(ret))\n",
    "    ret = [ret[k] for k in order]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = 1\n",
    "\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 5\n",
    "num_hidden = 110\n",
    "\n",
    "tests_hu_words = read_hu_en_data(\"hu_en_datas/hu_tests.txt\")\n",
    "tests_en_words = read_hu_en_data(\"hu_en_datas/en_tests.txt\")\n",
    "\n",
    "tests_data_hu = create_input_from_words(tests_hu_words, padding=30)\n",
    "tests_data_en = create_input_from_words(tests_en_words, padding=30)\n",
    "\n",
    "valid_hu_words = read_hu_en_data(\"hu_en_datas/hu_valid.txt\")\n",
    "valid_en_words = read_hu_en_data(\"hu_en_datas/en_valid.txt\")\n",
    "train_hu_words = read_hu_en_data(\"hu_en_datas/hu_train.txt\")\n",
    "train_en_words = read_hu_en_data(\"hu_en_datas/en_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_param = \"HU0EN100\"\n",
    "print(train_param)\n",
    "\n",
    "model_en = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "earlyStopping_cnn = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "valid_words = valid_en_words[:200*scale]\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = train_en_words[:1000*scale]\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_en.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_en,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_en,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "\n",
    "train_param = \"HU20EN80\"\n",
    "print(train_param)\n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "valid_words = shuffle_concat_2D(valid_en_words[:160*scale], valid_hu_words[:40*scale])\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = shuffle_concat_2D(train_en_words[:800*scale], train_hu_words[:200*scale])\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_cnn.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "\n",
    "    \n",
    "train_param = \"HU50EN50\"\n",
    "print(train_param)    \n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "valid_words = shuffle_concat_2D(valid_en_words[:100*scale], valid_hu_words[:100*scale])\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = shuffle_concat_2D(train_en_words[:500*scale], train_hu_words[:500*scale])\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_cnn.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "\n",
    "    \n",
    "train_param = \"HU80EN20\"\n",
    "print(train_param)    \n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "valid_words = shuffle_concat_2D(valid_en_words[:20*scale], valid_hu_words[:160*scale])\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = shuffle_concat_2D(train_en_words[:200*scale], train_hu_words[:800*scale])\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_cnn.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "    \n",
    "train_param = \"HU90EN10\"\n",
    "print(train_param)    \n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "valid_words = shuffle_concat_2D(valid_en_words[:20*scale], valid_hu_words[:180*scale])\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = shuffle_concat_2D(train_en_words[:100*scale], train_hu_words[:900*scale])\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_cnn.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_cnn,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "    \n",
    "train_param = \"HU100EN0\"\n",
    "print(train_param)\n",
    "\n",
    "model_hu = model_creator_cnn(len(tag_chars), num_layers=2, num_hidden=1024,kernel_size=8)\n",
    "\n",
    "valid_words = valid_hu_words[:200*scale]\n",
    "valid_data = create_input_from_words(valid_words, padding=30)\n",
    "train_words = train_hu_words[:1000*scale]\n",
    "train_data = create_input_from_words(train_words, padding=30)\n",
    "\n",
    "history_cnn = model_hu.fit(train_data[\"input_cnn\"], train_data[\"target_cnn\"],\n",
    "                            epochs=10*scale, batch_size=1024,\n",
    "                            validation_data=(valid_data[\"input_cnn\"], valid_data[\"target_cnn\"]),\n",
    "                            verbose=0, callbacks=[earlyStopping_cnn])\n",
    "ev_hu = test_ev(model_hu,\"cnn\", verbose=0, tests_data=tests_data_hu)\n",
    "ev_en = test_ev(model_hu,\"cnn\", verbose=0, tests_data=tests_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)\n",
    "    \n",
    "\n",
    "train_param = \"LangSelect\"\n",
    "print(train_param)\n",
    "\n",
    "tests_hu_detect = []\n",
    "tests_en_detect = []\n",
    "detect_else = 0\n",
    "\n",
    "for word in tests_hu_words:\n",
    "    if ld.detect(word[0])=='hu':\n",
    "        tests_hu_detect.append(word)\n",
    "    elif ld.detect(word[0])=='en':\n",
    "        tests_en_detect.append(word)\n",
    "    else:\n",
    "        detect_else+=1\n",
    "for word in tests_en_words:\n",
    "    if ld.detect(word[0])=='hu':\n",
    "        tests_hu_detect.append(word)\n",
    "    elif ld.detect(word[0])=='en':\n",
    "        tests_en_detect.append(word)\n",
    "    else:\n",
    "        detect_else+=1\n",
    "train_param+= '\\tAll:' + str(len(tests_hu_words) + len(tests_en_words)) \n",
    "train_param+= '\\tHu:' + str(len(tests_hu_detect))\n",
    "train_param+= '\\tEn:' + str(len(tests_en_detect))\n",
    "train_param+= '\\tElse:'+ str(detect_else)\n",
    "\n",
    "detect_data_hu = create_input_from_words(tests_hu_detect, padding=30)\n",
    "detect_data_en = create_input_from_words(tests_en_detect, padding=30)\n",
    "\n",
    "ev_hu = test_ev(model_hu,\"cnn\", verbose=0, tests_data=detect_data_hu)\n",
    "ev_en = test_ev(model_en,\"cnn\", verbose=0, tests_data=detect_data_en)\n",
    "\n",
    "with open(\"results_HU_EN.txt\", \"a\") as myfile:\n",
    "    result = train_param+'\\n'+'HU:\\t'+str(ev_hu)+'\\nEN:\\t'+str(ev_en)+'\\n'\n",
    "    myfile.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating HU_EN_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padding = 30\n",
    "tail_cut = 2000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 3\n",
    "num_hidden = 150\n",
    "\n",
    "train_num = 1000\n",
    "valid_num = 200\n",
    "tests_num = 100\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "counter_en_data = data_reader('umbc_word_freqs_2.txt',tail_cut, lang_selector=False)\n",
    "\n",
    "datas, wrong_words, long_word = train_data_words(counter_hu_data, tag_chars,\n",
    "                                                 padding, tail_cut, language='hu',\n",
    "                                                no_split=True)\n",
    "datas2, wrong_words, long_word = train_data_words(counter_en_data, tag_chars,\n",
    "                                                 padding, tail_cut, language='en',\n",
    "                                                no_split=True)\n",
    "\n",
    "hu_words = datas[\"words\"]\n",
    "en_words = datas2[\"words\"]\n",
    "\n",
    "hu_words_wrong = np.ones((np.shape(hu_words)[0],1), dtype=bool)\n",
    "en_words_wrong = np.ones((np.shape(en_words)[0],1), dtype=bool)\n",
    "filter_count=0\n",
    "hu_words_f = []\n",
    "hu_input_f = []\n",
    "hu_target_f = []\n",
    "en_words_f = []\n",
    "en_input_f = []\n",
    "en_target_f = []\n",
    "\n",
    "for i in range(len(hu_words)):\n",
    "    for j in range(len(en_words)):\n",
    "        if len(hu_words[i])==len(en_words[j]):\n",
    "            if (hu_words[i,0] == en_words[j,0]):\n",
    "                if (hu_words[i,1] != en_words[j,1]):\n",
    "                    hu_words_wrong[i]=0\n",
    "                    en_words_wrong[j]=0\n",
    "                    filter_count +=1\n",
    "                    #print(hu_words[i],en_words[j])\n",
    "\n",
    "            \n",
    "for i in range(len(hu_words)):\n",
    "    if hu_words_wrong[i] == 1:\n",
    "        hu_words_f.append(hu_words[i])\n",
    "for i in range(len(en_words)):\n",
    "    if en_words_wrong[i] == 1:\n",
    "        en_words_f.append(en_words[i])\n",
    "        \n",
    "with open(\"hu_en_datas/hu_train.txt\", \"w\") as myfile:\n",
    "    for i in range(0,train_num):\n",
    "        myfile.write(hu_words_f[i][0]+'\\t'+hu_words_f[i][1]+'\\n')\n",
    "with open(\"hu_en_datas/hu_valid.txt\", \"w\") as myfile:\n",
    "    for i in range(train_num,train_num+valid_num):\n",
    "        myfile.write(hu_words_f[i][0]+'\\t'+hu_words_f[i][1]+'\\n') \n",
    "with open(\"hu_en_datas/hu_tests.txt\", \"w\") as myfile:\n",
    "    for i in range(train_num+valid_num,train_num+valid_num+tests_num):\n",
    "        myfile.write(hu_words_f[i][0]+'\\t'+hu_words_f[i][1]+'\\n') \n",
    "with open(\"hu_en_datas/en_train.txt\", \"w\") as myfile:\n",
    "    for i in range(0,train_num):\n",
    "        myfile.write(en_words_f[i][0]+'\\t'+en_words_f[i][1]+'\\n')\n",
    "with open(\"hu_en_datas/en_valid.txt\", \"w\") as myfile:\n",
    "    for i in range(train_num,train_num+valid_num):\n",
    "        myfile.write(en_words_f[i][0]+'\\t'+en_words_f[i][1]+'\\n') \n",
    "with open(\"hu_en_datas/en_tests.txt\", \"w\") as myfile:\n",
    "    for i in range(train_num+valid_num,train_num+valid_num+tests_num):\n",
    "        myfile.write(en_words_f[i][0]+'\\t'+en_words_f[i][1]+'\\n') \n",
    "            \n",
    "print(np.shape(hu_words), np.shape(hu_words_f), filter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    " - https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padding = 30\n",
    "padding_with_inserted = 40\n",
    "input_characters = set('aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz')\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = set('aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz^$-')\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "max_encoder_seq_length = padding\n",
    "max_decoder_seq_length = padding_with_inserted\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def generate_seq2seq_data(words,\n",
    "                          hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    # Vectorize the data.\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for word in words:\n",
    "        input_text = word[0]\n",
    "        target_text = hypher.inserted(input_text)\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        target_text = '^' + target_text + '$'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                raise ValueError(\"Character not in input_charset: \"+char)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                raise ValueError(\"Character not in output_charset: \"+char)\n",
    "\n",
    "\n",
    "    print('Number of samples:', len(input_texts))\n",
    "\n",
    "\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    datas = {}\n",
    "    datas[\"encoder_input_data\"] = encoder_input_data\n",
    "    datas[\"decoder_input_data\"] = decoder_input_data\n",
    "    datas[\"decoder_target_data\"] = decoder_target_data\n",
    "    return datas\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['^']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '$' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "# Summary of the algorithm\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and correspding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "# Data download\n",
    "English to French sentence pairs.\n",
    "http://www.manythings.org/anki/fra-eng.zip\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "http://www.manythings.org/anki/\n",
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 1024  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "\n",
    "hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "\n",
    "\n",
    "train_data = generate_seq2seq_data(train_words)\n",
    "encoder_input_data = train_data[\"encoder_input_data\"]\n",
    "decoder_input_data = train_data[\"decoder_input_data\"]\n",
    "decoder_target_data = train_data[\"decoder_target_data\"]\n",
    "\n",
    "valid_data = generate_seq2seq_data(valid_words)\n",
    "valid_encoder_input_data = valid_data[\"encoder_input_data\"]\n",
    "valid_decoder_input_data = valid_data[\"decoder_input_data\"]\n",
    "valid_decoder_target_data = valid_data[\"decoder_target_data\"]\n",
    "\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "earlyStopping_seq2seq = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=7, verbose=0, mode='auto')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([valid_data[\"encoder_input_data\"],\n",
    "                            valid_data[\"decoder_input_data\"]],\n",
    "                           valid_data[\"decoder_target_data\"]),\n",
    "          callbacks=[earlyStopping_seq2seq])\n",
    "# Save model\n",
    "model.save('models/s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tests_data = generate_seq2seq_data(tests_words)\n",
    "tests_encoder_input_data = tests_data[\"encoder_input_data\"]\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = tests_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', tests_words[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
