{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, Embedding, LSTM\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext pep8_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hun_chars = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz' + '^$'  # ^,$\n",
    "\n",
    "\n",
    "def hyph_tags(word, hypher=pyphen.Pyphen(lang='hu_HU'), aslist=False):\n",
    "    \"\"\"Hyphenating classification of the characters in the word.\n",
    "    {B(egin),M(iddle),E(nd),S(ingle)}\"\"\"\n",
    "    if (len(word) == 0):\n",
    "        raise IndexError(\"0 length word\")\n",
    "    ret = list('M' * len(word))\n",
    "    ret[0] = 'B'\n",
    "    ret[-1] = 'E'\n",
    "    for i in hypher.positions(word):\n",
    "        ret[i] = 'B'\n",
    "        if(ret[i-1] == 'B'):\n",
    "            ret[i-1] = 'S'\n",
    "        else:\n",
    "            ret[i-1] = 'E'\n",
    "    if (aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def hyph_tags_4to2(word, aslist=False):\n",
    "    \"\"\"{B,M,E,S} to {B, M}\"\"\"\n",
    "    ret = list(word)\n",
    "    for i in range(len(ret)):\n",
    "        if ret[i] == 'S':\n",
    "            ret[i] = 'B'\n",
    "        if ret[i] != 'B':\n",
    "            ret[i] = 'M'\n",
    "    if(aslist):\n",
    "        return ret\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "def same_char_num(word, hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    \"\"\"Return true if the hyphenated word has as many chars as the original\"\"\"\n",
    "    return len(hypher.inserted(word)) == len(word)+len(hypher.positions(word))\n",
    "\n",
    "\n",
    "def cleaning(data):\n",
    "    \"\"\"Text cleaning:\n",
    "        lower the letters\n",
    "        punctuation, digits ellimination\"\"\"\n",
    "    formated_data = data.lower()\n",
    "    formated_data = re.sub('['+string.punctuation+']', '', formated_data)\n",
    "    formated_data = re.sub('['+string.digits+']', '', formated_data)\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# onehot: {'B','M','E','S'}\n",
    "def one_hot_encode(char, dictionary='BMES'):\n",
    "    ret = [0]*len(dictionary)\n",
    "    if char in dictionary:\n",
    "        ret[dictionary.find(char)] = 1\n",
    "        return ret\n",
    "    raise ValueError('Value out of dictionary range: '+char)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    \"\"\"Randomize 2 same length array in the same permutation\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def one_hot_decode(arr, dictionary='BMES'):\n",
    "    assert len(arr) == len(dictionary)\n",
    "    i = np.nonzero(arr)[0][0]\n",
    "    return dictionary[i]\n",
    "\n",
    "\n",
    "def generate_network_data(data, ret_input=[], ret_output=[],\n",
    "                          length=2, length_after=0,\n",
    "                          start_char='^', end_char='$',\n",
    "                          chars=hun_chars, tag_chars='BMES'):\n",
    "    \"\"\"from [word,hyph_class(word) to length-long input-output data\"\"\"\n",
    "    word = data[0]\n",
    "    word_plus = start_char*(length-length_after-1)+word+end_char*length_after\n",
    "    hyph_word = data[1]\n",
    "    for i in range(0, len(word)):\n",
    "        input_next_iter = []\n",
    "        for c in word_plus[i:i+length]:\n",
    "            input_next_iter.append(one_hot_encode(c, chars))\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return\n",
    "\n",
    "def generate_network_words(data, padding=None, start_char='^',\n",
    "                           end_char='$', chars=hun_chars,\n",
    "                           tag_chars='BMES', tag_default=-1):\n",
    "    \"\"\"One-hot [word, hyph_class(word)]->[[[010],[010]],[[01],[01]]]\n",
    "    padding to fixed size, if not null\"\"\"\n",
    "    ret_input=[]\n",
    "    ret_output=[]\n",
    "    \n",
    "    word = data[0]\n",
    "    hyph_word = data[1]\n",
    "    if padding != None:\n",
    "        if len(word)>padding:\n",
    "            raise IndexError(\"The word is longer than the fixed size\")\n",
    "        else:\n",
    "            word = word + (padding-len(word))*end_char\n",
    "            hyph_word = hyph_word + (padding-len(hyph_word)) * tag_chars[tag_default]\n",
    "    for i in range(0,len(word)):\n",
    "        input_next_iter = one_hot_encode(word[i],chars)\n",
    "        output_next_iter = one_hot_encode(hyph_word[i], tag_chars)\n",
    "        ret_input.append(input_next_iter)\n",
    "        ret_output.append(output_next_iter)\n",
    "    return ret_input, ret_output\n",
    "    \n",
    "def hyph_tupples(data, hypher=pyphen.Pyphen(lang='hu_HU'),\n",
    "                tag_chars='BM'):\n",
    "    \"\"\"[words] -> [words, hyph_words]\"\"\"\n",
    "    word_list = []\n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    for next_word in data:\n",
    "        c_all += 1\n",
    "        if(len(next_word) != 0 and same_char_num(next_word, hypher)):\n",
    "            c_same_char_num += 1\n",
    "            if(len(tag_chars) == 2):\n",
    "                word_list.append([next_word,\n",
    "                                  hyph_tags_4to2(hyph_tags(next_word))])\n",
    "            else:\n",
    "                word_list.append([next_word, hyph_tags(next_word)])\n",
    "    return word_list, c_all, c_same_char_num\n",
    "\n",
    "def tupple_to_train(word_list, window_length, length_after,\n",
    "                 tag_chars='BM'):\n",
    "    \"\"\"[words, hyph_words] -> in[0,1,0...], out[0,1,0...]\"\"\"\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            generate_network_data(word, data_in, data_out,\n",
    "                                  window_length, tag_chars=tag_chars,\n",
    "                                  length_after=length_after)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "    return data_in, data_out, wrong_word\n",
    "\n",
    "def bigram_counter_from_file(filename):\n",
    "    \"\"\"creates bigram counter from file\"\"\"\n",
    "    with open(filename) as f:\n",
    "        word_list = []\n",
    "        for words in f:\n",
    "            words = words.strip()\n",
    "            words = words.split()\n",
    "            for w in words:\n",
    "                w = cleaning(w)\n",
    "                if len(w)>0:\n",
    "                    word_list.append(w)\n",
    "\n",
    "    bigram_counter = collections.Counter()\n",
    "    for word in word_list:\n",
    "        for i in range(2,len(word)):\n",
    "            bigram_counter[word[i-2:i]] += 1\n",
    "    return bigram_counter\n",
    "\n",
    "def bigrams_in_word(word, bigram_counter, mc=100):\n",
    "    bigrams = np.array(bigram_counter.most_common(mc))[:,0]\n",
    "    w_bc = len(word)-1\n",
    "    if w_bc<1:\n",
    "        return 1.0\n",
    "    w_bf = 0\n",
    "    for i in range(2,len(word)):\n",
    "        if word[i-2:i] in bigrams:\n",
    "            w_bf +=1\n",
    "    return w_bf/w_bc\n",
    "\n",
    "def bigram_selector(word, bigram_counters,threshold=0.2, mc=100):\n",
    "    \"\"\"Choose the language of the word\"\"\"\n",
    "    lang_likes = np.zeros(len(bigram_counters)+1)\n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        lang_likes[i] = bigrams_in_word(word, bigram_counters[i], mc)\n",
    "    lang_likes_max = np.argmax(lang_likes)\n",
    "    \n",
    "    for i in range(0,len(bigram_counters)):\n",
    "        if i!=lang_likes_max:\n",
    "            if lang_likes[lang_likes_max]-lang_likes[i]<=threshold:\n",
    "                return len(bigram_counters)\n",
    "    return lang_likes_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_reader(file, tail_cut=100000,\n",
    "                lang_selector = False, lang_thr=0.6,\n",
    "                lang_file_en='../wikipedia/angol/ossz_angol',\n",
    "                lang_file_hu='../wikipedia/magyar/ossz_magyar'):\n",
    "    \"\"\"Read data from file\"\"\"\n",
    "\n",
    "    if lang_selector:\n",
    "        bigram_counter_en = bigram_counter_from_file(lang_file_en)\n",
    "        bigram_counter_hu = bigram_counter_from_file(lang_file_hu)\n",
    "        out_en_words = 0\n",
    "    \n",
    "    tail_cut_ptest_words = tail_cut + 500\n",
    "\n",
    "    counter_hu_data = collections.Counter()\n",
    "    with open(file, 'r',\n",
    "              errors='ignore', encoding='latin2') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i = i+1\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if(words[1].isdigit()):\n",
    "                    cword = cleaning(words[0])\n",
    "                    if lang_selector:\n",
    "                        lang = bigram_selector(cword,\n",
    "                                            [bigram_counter_hu,\n",
    "                                             bigram_counter_en],\n",
    "                                            lang_thr)\n",
    "                        if (lang!=1):\n",
    "                            counter_hu_data[cword] += int(words[1])\n",
    "                        else:\n",
    "                            out_en_words +=1\n",
    "                    else:\n",
    "                        counter_hu_data[cword] += int(words[1])\n",
    "            if i > tail_cut_ptest_words:\n",
    "                break\n",
    "    if lang_selector:\n",
    "        print(\"Throwed english words: \", out_en_words)\n",
    "    return counter_hu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_generator(data_counter, window_length, length_after,\n",
    "                         tag_chars='BM', tail_cut=100000,\n",
    "                         valid_rate=0.2, test_rate=0.1):\n",
    "    \"\"\"Generate training data from counter data\n",
    "    unique words -> characters -> randomize -> cut\"\"\"\n",
    "\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "\n",
    "\n",
    "def train_data_generator_uwords(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> randomize -> cut -> characters\"\"\"\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    np.random.shuffle(data_list)\n",
    "    data_len = len(data_list)\n",
    "    tests_data = data_list[0:int(data_len*test_rate)]\n",
    "    valid_data = data_list[int(data_len*test_rate):\n",
    "                           int(data_len*(test_rate+valid_rate))]\n",
    "    train_data = data_list[int(data_len*(test_rate+valid_rate)):]\n",
    "    \n",
    "    c_all = 0\n",
    "    c_same_char_num = 0\n",
    "    tests_list, c_all_p, c_same_char_num_p = hyph_tupples(tests_data,\n",
    "                                                          tag_chars=tag_chars)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    valid_list, c_all_p, c_same_char_num_p = hyph_tupples(valid_data,\n",
    "                                                          tag_chars=tag_chars)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    train_list, c_all_p, c_same_char_num_p = hyph_tupples(train_data,\n",
    "                                                          tag_chars=tag_chars)\n",
    "    c_all += c_all_p\n",
    "    c_same_char_num += c_same_char_num_p\n",
    "    \n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    wrong_word = 0\n",
    "    tests_input, tests_target, wrong_w_p = tupple_to_train(tests_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    valid_input, valid_target, wrong_w_p = tupple_to_train(valid_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    train_input, train_target, wrong_w_p = tupple_to_train(train_list,\n",
    "                                                           window_length,\n",
    "                                                           length_after,\n",
    "                                                           tag_chars=tag_chars)\n",
    "    wrong_word += wrong_w_p\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    \n",
    "\n",
    "def train_data_generator_uchars(data_counter, window_length, length_after,\n",
    "                                tag_chars='BM', tail_cut=100000,\n",
    "                                valid_rate=0.2, test_rate=0.1):\n",
    "    \"\"\"Generate training data from counter data\n",
    "        unique words -> characters -> unique -> randomize -> cut\"\"\"\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "\n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    wrong_word = 0\n",
    "    data_in, data_out, wrong_word = tupple_to_train(word_list,\n",
    "                                                    window_length,\n",
    "                                                    length_after,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "\n",
    "    #Unique\n",
    "    data_len = len(data_in)\n",
    "\n",
    "    data_in = np.array(data_in, dtype='float32')\n",
    "    data_out = np.array(data_out, dtype='float32')\n",
    "    \n",
    "    shape_in = np.shape(data_in)\n",
    "    shape_out = np.shape(data_out)\n",
    "    \n",
    "    data_in_flatten = np.reshape(\n",
    "        data_in, (shape_in[0], shape_in[1]*shape_in[2]))\n",
    "    shape_in_flatten = np.shape(data_in_flatten)\n",
    "    \n",
    "    data_iosum = np.concatenate((data_in_flatten, data_out), axis=1)\n",
    "    data_iosum_unique = np.vstack({tuple(row) for row in data_iosum})\n",
    "    \n",
    "    data_in = data_iosum_unique[:,:-shape_out[1]]\n",
    "    data_out = data_iosum_unique[:,-shape_out[1]:]\n",
    "    print('Data unique len: ', np.shape(data_iosum_unique)[0])\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    data_in, data_out = unison_shuffled_copies(data_in, data_out)\n",
    "    tests_input = data_in[0:int(data_len*test_rate)]\n",
    "    tests_target = data_out[0:int(data_len*test_rate)]\n",
    "    valid_input = data_in[int(data_len*test_rate):\n",
    "                          int(data_len*(test_rate+valid_rate))]\n",
    "    valid_target = data_out[int(data_len*test_rate):\n",
    "                            int(data_len*(test_rate+valid_rate))]\n",
    "    train_input = data_in[int(data_len*(test_rate+valid_rate)):]\n",
    "    train_target = data_out[int(data_len*(test_rate+valid_rate)):]\n",
    "\n",
    "    print('Training data size:', np.shape(train_input), np.shape(train_target))\n",
    "    print('Validation data size:', np.shape(valid_input),\n",
    "          np.shape(valid_target))\n",
    "    print('Test data size:', np.shape(tests_input), np.shape(tests_target))\n",
    "\n",
    "    train_input_flatten = np.reshape(\n",
    "        train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "    valid_input_flatten = np.reshape(\n",
    "        valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "    tests_input_flatten = np.reshape(\n",
    "        tests_input, (len(tests_input), (window_length)*len(hun_chars)))\n",
    "    print('Network data generated successfully')\n",
    "\n",
    "    return [train_input_flatten, train_target,\n",
    "            valid_input_flatten, valid_target,\n",
    "            tests_input_flatten, tests_target]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_words(data_counter, tag_chars='BM', padding = 30, tail_cut=100000,\n",
    "                     valid_rate=0.2, test_rate=0.1):\n",
    "    \"\"\"Training data, example: alma -> {[[1,0..][0,0..][0,0...][0,0...]],[[1,0],[0,1][1,0][0,1]]}\"\"\"\n",
    "    data_list = np.array(data_counter.most_common(tail_cut))[:,0]\n",
    "    word_list, c_all, c_same_char_num = hyph_tupples(data_list,\n",
    "                                                    tag_chars=tag_chars)\n",
    "    print('Data read successfully')\n",
    "    print('non-standard hyphenation:')\n",
    "    print(c_same_char_num, c_all, c_same_char_num/c_all)\n",
    "    \n",
    "    # Generate network data\n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    data_words = []\n",
    "    wrong_word = 0\n",
    "    long_word = 0\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            next_data_in, next_data_out = generate_network_words(word, padding = padding, tag_chars=tag_chars)\n",
    "            next_data_in = np.array(next_data_in, dtype='float32')\n",
    "            next_data_out = np.array(next_data_out, dtype='float32')\n",
    "            data_in.append(next_data_in)\n",
    "            data_out.append(next_data_out)\n",
    "            data_words.append(word)\n",
    "        except ValueError:\n",
    "            wrong_word += 1\n",
    "        except IndexError:\n",
    "            long_word += 1\n",
    "            \n",
    "    print('Data len: ', len(data_in))\n",
    "    print('Words with unrecognized caracter: ', wrong_word)\n",
    "    print('Words longer than the padding: ', long_word)\n",
    "    \n",
    "    data_in = np.array(data_in)\n",
    "    data_out = np.array(data_out)\n",
    "    \n",
    "    data_len = len(data_in)\n",
    "    order = np.random.permutation(data_len)\n",
    "    data_in = [data_in[k] for k in order]\n",
    "    data_out = [data_out[k] for k in order]\n",
    "    data_words = [data_words[k] for k in order]\n",
    "    \n",
    "    #data_in, data_out, word_list = unison_shuffled_copies(data_in, data_out, word_list)\n",
    "    \n",
    "    datas = {}\n",
    "    \n",
    "    datas[\"tests_words\"] = np.array(data_words[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_input\"] = np.array(data_in[0:int(data_len*test_rate)])\n",
    "    datas[\"tests_target\"] = np.array(data_out[0:int(data_len*test_rate)])\n",
    "    datas[\"valid_words\"] = np.array(data_words[int(data_len*test_rate):\n",
    "                                               int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_input\"] = np.array(data_in[int(data_len*test_rate):\n",
    "                                            int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"valid_target\"] = np.array(data_out[int(data_len*test_rate):\n",
    "                                              int(data_len*(test_rate+valid_rate))])\n",
    "    datas[\"train_words\"] = np.array(data_words[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_input\"] = np.array(data_in[int(data_len*(test_rate+valid_rate)):])\n",
    "    datas[\"train_target\"] = np.array(data_out[int(data_len*(test_rate+valid_rate)):])\n",
    "    \n",
    "    return datas, wrong_word, long_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_creator_dnn(window_length, output_length, num_layers=1,\n",
    "                  num_hidden=10, chars=hun_chars):\n",
    "    \"\"\"Creates Keras model with the given input, output dimensions\n",
    "    and layer number, hidden layer length\"\"\"\n",
    "    \n",
    "    input_shape = window_length*len(chars)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(input_dim=(input_shape),\n",
    "                    units=num_hidden, name='input_layer',\n",
    "                    activation='sigmoid'))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(units=num_hidden, activation='sigmoid'))\n",
    "\n",
    "    # model.add(Flatten())\n",
    "    model.add(Dense(output_length, name='output_layer', activation='softmax'))\n",
    "\n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_cnn(output_length,kernel_size=10, strides=1, word_length = 30, chars=hun_chars):\n",
    "    \"\"\"Creates Keras CNN model\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(516,kernel_size, strides=strides, padding=\"same\",\n",
    "                     activation='relu', input_shape=(word_length, len(chars))))\n",
    "    \n",
    "\n",
    "\n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def model_creator_lstm(output_length, word_length = 30, chars = hun_chars):\n",
    "    \"\"\"Creates Keras LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(128, activation='relu',return_sequences=True,\n",
    "                   go_backwards=True,\n",
    "                   input_shape=(word_length, len(chars))))\n",
    "    model.add(LSTM(64, activation='relu',return_sequences=True,\n",
    "                   go_backwards=True,))\n",
    "    \n",
    "    model.add(Dense((output_length), name = 'output_layer', activation='softmax'))\n",
    "\n",
    "    \n",
    "    if(output_length == 2):\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully\n",
      "non-standard hyphenation:\n",
      "82563 83678 0.9866751117378523\n",
      "Data len:  81910\n",
      "Words with unrecognized caracter:  646\n",
      "Words longer than the padding:  7\n",
      "(57337, 30, 37) (16382, 30, 37) (8191, 30, 37)\n"
     ]
    }
   ],
   "source": [
    "padding = 30\n",
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 5\n",
    "num_hidden = 110\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "\n",
    "datas, wrong_words, long_word = train_data_words(counter_hu_data, tag_chars,\n",
    "                                                             padding, tail_cut)\n",
    "\n",
    "tests_input_cnn = datas[\"tests_input\"]\n",
    "tests_target_cnn = datas[\"tests_target\"]\n",
    "valid_input_cnn = datas[\"valid_input\"]\n",
    "valid_target_cnn = datas[\"valid_target\"]\n",
    "train_input_cnn = datas[\"train_input\"]\n",
    "train_target_cnn = datas[\"train_target\"]\n",
    "\n",
    "tests_words = datas[\"tests_words\"]\n",
    "valid_words = datas[\"valid_words\"]\n",
    "train_words = datas[\"train_words\"]\n",
    "\n",
    "\n",
    "print(np.shape(train_input_cnn), np.shape(valid_input_cnn), np.shape(tests_input_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still wrong word (expected zero):  0\n"
     ]
    }
   ],
   "source": [
    "wrong_word = 0\n",
    "tests_input, tests_target, wrong_w_p = tupple_to_train(tests_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "valid_input, valid_target, wrong_w_p = tupple_to_train(valid_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "train_input, train_target, wrong_w_p = tupple_to_train(train_words,\n",
    "                                                        window_length,\n",
    "                                                        length_after,\n",
    "                                                        tag_chars=tag_chars)\n",
    "wrong_word += wrong_w_p\n",
    "\n",
    "print(\"Still wrong word (expected zero): \", wrong_word)\n",
    "\n",
    "train_input_flatten = np.reshape(\n",
    "    train_input, (len(train_input), (window_length)*len(hun_chars)))\n",
    "valid_input_flatten = np.reshape(\n",
    "    valid_input, (len(valid_input), (window_length)*len(hun_chars)))\n",
    "tests_input_flatten = np.reshape(\n",
    "    tests_input, (len(tests_input), (window_length)*len(hun_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully\n",
      "non-standard hyphenation:\n",
      "82563 83678 0.9866751117378523\n",
      "Data len:  698309\n",
      "Words with unrecognized caracter:  646\n",
      "Training data size: (488817, 7, 37) (488817, 2)\n",
      "Validation data size: (139662, 7, 37) (139662, 2)\n",
      "Test data size: (69830, 7, 37) (69830, 2)\n",
      "Network data generated successfully\n"
     ]
    }
   ],
   "source": [
    "tail_cut = 100000\n",
    "window_length = 7\n",
    "length_after = 3\n",
    "tag_chars = 'BM'\n",
    "num_layers = 5\n",
    "num_hidden = 110\n",
    "\n",
    "# Data read and network data generate\n",
    "counter_hu_data = data_reader('web2.2-freq-sorted.txt',tail_cut, lang_selector=False)\n",
    "[train_input_flatten, train_target,\n",
    " valid_input_flatten, valid_target,\n",
    " tests_input_flatten,\n",
    " tests_target] = train_data_generator(counter_hu_data,\n",
    "                                             window_length,\n",
    "                                             length_after,\n",
    "                                             tag_chars,\n",
    "                                             tail_cut)\n",
    "\n",
    "#train_input_flatten_p1 = np.expand_dims(train_input_flatten, axis=1) # reshape (X, 1, 259) \n",
    "#valid_input_flatten_p1 = np.expand_dims(valid_input_flatten, axis=1)\n",
    "#tests_input_flatten_p1 = np.expand_dims(tests_input_flatten, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models created. Start training\n"
     ]
    }
   ],
   "source": [
    "# Creating the keras model\n",
    "model_dnn = model_creator_dnn(window_length, len(tag_chars),\n",
    "                      num_layers, num_hidden)\n",
    "\n",
    "earlyStopping_dnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0, mode='auto')\n",
    "\n",
    "model_cnn = model_creator_cnn(len(tag_chars), kernel_size=15)\n",
    "\n",
    "earlyStopping_cnn = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "model_lstm = model_creator_lstm(len(tag_chars))\n",
    "\n",
    "earlyStopping_lstm = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "\n",
    "print('Models created. Start training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_lstm, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=0, callbacks=[earlyStopping_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_cnn = model_cnn.fit(train_input_cnn, train_target_cnn,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_cnn, valid_target_cnn),\n",
    "                    verbose=0, callbacks=[earlyStopping_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.fit(train_input_flatten, train_target,\n",
    "                    epochs=1000, batch_size=1024,\n",
    "                    validation_data=(valid_input_flatten, valid_target),\n",
    "                    verbose=0, callbacks=[earlyStopping_dnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bac7a0118b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# summarize history for loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_cnn' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9568394be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(history_cnn.history['loss'])\n",
    "plt.plot(history_cnn.history['val_loss'])\n",
    "\n",
    "plt.plot(history_lstm.history['loss'])\n",
    "plt.plot(history_lstm.history['val_loss'])\n",
    "plt.plot(history_dnn.history['loss'])\n",
    "plt.plot(history_dnn.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cnn_train', 'cnn_validation',\n",
    "            'lstm_train', 'lstm_validation',\n",
    "            'ffnn_train', 'ffnn_validation'], loc='upper right')\n",
    "#plt.ylim((0.0, 0.02))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dnn.save('models/mBMdnn3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dnn = keras.models.load_model('models/mBMdnnB.h5')\n",
    "model_cnn = keras.models.load_model('models/mBMcnnB.h5')\n",
    "model_lstm = keras.models.load_model('models/mBMlstmB.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyph_predict(word, model,\n",
    "                 length=2, length_after=0, tag_chars='BMES', aslist=False, model_type='dnn'):\n",
    "    \"\"\"Generate tagging from the input word according to the model\"\"\"\n",
    "    word_in = []\n",
    "    word_out = []\n",
    "    generate_network_data([word, len(word)*tag_chars[0]],\n",
    "                          word_in, word_out, length=length,\n",
    "                          length_after=length_after, tag_chars=tag_chars)\n",
    "    word_in = np.reshape(word_in, (len(word_in), (length)*len(hun_chars)))\n",
    "    if model_type=='cnn':\n",
    "        word_in = np.expand_dims(word_in, axis=1) # reshape (x, 1, 259) \n",
    "    word_out = model.predict(word_in)\n",
    "    tag_list = np.array(list(tag_chars))\n",
    "    temp = np.argmax(word_out, axis=1)\n",
    "    temp = tag_list[temp]\n",
    "    if(aslist):\n",
    "        return temp\n",
    "    return \"\".join(temp)\n",
    "\n",
    "\n",
    "def hyph_insterted(word, model,\n",
    "                   length=2, length_after=0, tag_chars='BMES', model_type='dnn'):\n",
    "    tags = hyph_predict(word, model,length,\n",
    "                        length_after, tag_chars, aslist=False, model_type=model_type)\n",
    "    word_inserted = \"\"\n",
    "    if tag_chars=='BM':\n",
    "        for i in range(len(word)):\n",
    "            if i != 0 and tags[i]=='B':\n",
    "                word_inserted += '-'\n",
    "            word_inserted += word[i]\n",
    "    else:\n",
    "        raise NotImplementedError('BM implemented only')\n",
    "    return word_inserted\n",
    "\n",
    "\n",
    "def evaluation(wtags_predicted, wtags_target, tag_chars='BM'): \n",
    "    \"\"\"Compare BMBM with BBMB\"\"\"\n",
    "    if tag_chars!='BM':\n",
    "        raise NotImplementedError(\"Only BM available\")\n",
    "    tp = 0 # target: B prediction: B\n",
    "    tn = 0 # target: M prediction: M\n",
    "    fp = 0 # target: M prediction: B\n",
    "    fn = 0 # target: B prediction: M\n",
    "    for i in range(min(len(wtags_target),len(wtags_predicted))):\n",
    "        c_t = wtags_target[i]\n",
    "        c_p = wtags_predicted[i]\n",
    "        if (c_t == 'B') and (c_p == 'B'):\n",
    "            tp +=1\n",
    "        elif (c_t == 'M') and (c_p == 'M'):\n",
    "            tn +=1\n",
    "        elif (c_t == 'M') and (c_p == 'B'):\n",
    "            fp +=1\n",
    "        elif (c_t == 'B') and (c_p == 'M'):\n",
    "            fn +=1\n",
    "        else:\n",
    "            raise ValueError(\"Not expected tag!\" + c_t + c_p)\n",
    "    good = False\n",
    "    if fn+fp == 0:\n",
    "        good = True\n",
    "    return tp, tn, fp, fn, good\n",
    "\n",
    "\n",
    "def test_ev(model, model_type, model_params = None, num_tests=-1, verbose=1,\n",
    "            hypher=pyphen.Pyphen(lang='hu_HU')):\n",
    "    \"\"\"Evaulate the tests\"\"\"\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    good = 0\n",
    "    if (verbose>0):\n",
    "        print(\"Prediction\\tTarget\")\n",
    "    if model_type == 'cnn' or model_type =='lstm':\n",
    "        test_result = model.predict(tests_input_cnn[0:num_tests])\n",
    "        for i in range(len(test_result)):\n",
    "            test_tags = result_decode(test_result[i])\n",
    "            test_word = tests_words[i,0]\n",
    "            ev = evaluation(test_tags, tests_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "                \n",
    "\n",
    "    if model_type == 'dnn':\n",
    "        test_range = num_tests\n",
    "        if num_tests==-1:\n",
    "            test_range = len(tests_words)\n",
    "        window_length = model_params[\"window_length\"]\n",
    "        length_after = model_params[\"length_after\"]\n",
    "        tag_chars = model_params[\"tag_chars\"]\n",
    "        for i in range(test_range):\n",
    "            test_word = tests_words[i,0]\n",
    "            test_tags = hyph_predict(test_word, model_dnn,\n",
    "                                       window_length, length_after,\n",
    "                                       tag_chars,\n",
    "                                       model_type=model_type)\n",
    "            ev = evaluation(test_tags, tests_words[i,1])\n",
    "            tp += ev[0]\n",
    "            tn += ev[1]\n",
    "            fp += ev[2]\n",
    "            fn += ev[3]\n",
    "            if ev[4]:\n",
    "                good+=1\n",
    "            \n",
    "            if (verbose>0) and (ev[4] == False):\n",
    "                print(hyp_inserted(test_word,test_tags),'\\t',\n",
    "                      hypher.inserted(test_word))\n",
    "    \n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_Fscore = 2 * (test_precision *\n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    ret = {}\n",
    "    ret[\"precision\"] = test_precision\n",
    "    ret[\"recall\"] = test_recall\n",
    "    ret[\"Fscore\"] = test_Fscore\n",
    "    ret[\"word_rate\"] = good/len(tests_words)\n",
    "    return ret\n",
    "        \n",
    "    \n",
    "def result_decode(result, tag_chars='BM'):\n",
    "    \"\"\"[[0,1][0,1]] -> 'MM'\"\"\"\n",
    "    tags = \"\"\n",
    "    result = hardmax(result)\n",
    "    for c in result:\n",
    "        tags += one_hot_decode(c, tag_chars)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def hardmax(arr, axis = -1):\n",
    "    \"\"\"Return 1 if the value is the max in the row, 0 otherwise\n",
    "    [0.2,0.4,0.5]->[0,0,1]\"\"\"\n",
    "    temp = arr - np.max(arr, axis=axis, keepdims=True)\n",
    "    return np.round(1+temp)\n",
    "\n",
    "\n",
    "def hyp_inserted(word, tags, tag_chars='BM'):\n",
    "    \"\"\"insert hyphen to the tags\"\"\"\n",
    "    assert len(word)<=len(tags)\n",
    "    s = \"\"\n",
    "    for c in range(len(word)):\n",
    "        if (c!=0) and tags[c]=='B':\n",
    "            s+='-'\n",
    "        s+=word[c]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 8191 words.\n",
      "CNN: {'precision': 0.9877300613496932, 'word_rate': 0.9625198388475156, 'recall': 0.9970382337102854, 'Fscore': 0.9923623207825272}\n",
      "LSTM: {'precision': 0.9817491937013849, 'word_rate': 0.9479916982053473, 'recall': 0.9952304023386415, 'Fscore': 0.9884438332091763}\n",
      "FFNN: {'precision': 0.9882924589662164, 'word_rate': 0.9554388963496521, 'recall': 0.9935, 'Fscore': 0.9908893875750426}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Prediction \\t Target\")\n",
    "\n",
    "print(\"Evaluation on\",len(tests_words),\"words.\")\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"window_length\"] = window_length\n",
    "model_params[\"length_after\"] = length_after\n",
    "model_params[\"tag_chars\"] = tag_chars\n",
    "\n",
    "print(\"CNN:\",test_ev(model_cnn,\"cnn\", verbose=0))\n",
    "print(\"LSTM:\",test_ev(model_lstm,\"lstm\", verbose=0))\n",
    "print(\"FFNN:\",test_ev(model_dnn,\"dnn\",model_params=model_params, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69745, 2) (69745, 2)\n",
      "25709 281 43464 291\n"
     ]
    }
   ],
   "source": [
    "model_type = \"dnn\"\n",
    "\n",
    "test_tp = 0\n",
    "test_fp = 0\n",
    "test_tn = 0\n",
    "test_fn = 0\n",
    "test_str = \"\"\n",
    "\n",
    "if model_type == \"dnn\":\n",
    "    test_results = model_dnn.predict(tests_input_flatten)\n",
    "    history = history_dnn\n",
    "else:\n",
    "    if model_type == \"cnn\":\n",
    "        test_results = model_cnn.predict(tests_input_flatten_p1)\n",
    "        history = history_cnn\n",
    "if(np.shape(test_results)[1] == 2):\n",
    "    for i in range(len(test_results)):\n",
    "        # positive\n",
    "        if np.argmax(test_results[i]) == 1:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_tn += 1\n",
    "            else:\n",
    "                test_fn += 1\n",
    "        else:\n",
    "            if np.argmax(tests_target[i]) == 1:\n",
    "                test_fp += 1\n",
    "            else:\n",
    "                test_tp += 1\n",
    "    print(np.shape(test_results),np.shape(tests_target))\n",
    "    print(test_tp,test_fp,test_tn,test_fn)\n",
    "    test_precision = test_tp / (test_tp + test_fp)\n",
    "    test_recall = test_tp / (test_tp + test_fn)\n",
    "    test_Fscore = 2 * (test_precision * \n",
    "                       test_recall) / (test_precision + test_recall)\n",
    "    test_str = str(test_precision) + '\\t' + str(test_recall) \n",
    "    test_str += '\\t' + str(test_Fscore)\n",
    "else:\n",
    "    for i in range(len(test_results)):\n",
    "        if np.argmax(test_results[i]) == np.argmax(tests_target[i]):\n",
    "            test_success += 1\n",
    "        else:\n",
    "            test_fail += 1\n",
    "    test_str = str(test_fail/(test_fail+test_success))\n",
    "\n",
    "with open(\"results_data_types.txt\", \"a\") as myfile:\n",
    "    result = \"\"\n",
    "#    result += str(window_length) + '\\t'\n",
    "#    result += str(length_after) + '\\t' + tag_chars\n",
    "#    result += '\\t' + str(num_layers) + '\\t'\n",
    "#    result += str(num_hidden) + '\\t'\n",
    "    result += model_type + '\\t'\n",
    "    result += str(history.epoch[-1]) + '\\t'\n",
    "    result += str(history.history['val_loss'][-1])\n",
    "    result += '\\t' + test_str\n",
    "    result += '\\n'\n",
    "    myfile.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypher = pyphen.Pyphen(lang='hu_HU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 'szemüveg'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model, window_length, length_after, tag_chars, model_type='cnn'),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))\n",
    "\n",
    "test = 'leopárd'\n",
    "print('Word:', test, 'Prediction:',\n",
    "      hyph_predict(test, model, window_length, length_after, tag_chars, model_type='cnn'),\n",
    "      'Target:', hyph_tags_4to2(hyph_tags(test)), hypher.inserted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = counter_hu_data.most_common()[-400:]\n",
    "print('Prediction\\tTarget')\n",
    "for word in test_words:\n",
    "    next_word = word[0]\n",
    "    if(len(next_word) != 0 and same_char_num(next_word)):\n",
    "        try:\n",
    "            predicted_value = hyph_predict(next_word, model,\n",
    "                                           window_length, length_after,\n",
    "                                           tag_chars, model_type='cnn')\n",
    "            predicted_visual = hyph_insterted(next_word, model,\n",
    "                                              window_length, length_after,\n",
    "                                              tag_chars, model_type='cnn')\n",
    "            excepted_value = hyph_tags_4to2(hyph_tags(next_word))\n",
    "            success = predicted_value == excepted_value\n",
    "            if not success:\n",
    "                print(predicted_visual,\n",
    "                        '\\t',hypher.inserted(next_word))\n",
    "        except ValueError as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN\tCNN\tLSTM\tTarget\n",
      "pro-mo-ting\tpromo-ting\tpro-mo-ting\tpromo-ting\n",
      "köz-út\tköz-út\tkö-zút\tköz-út\n",
      "tan-évet\tta-n-é-vet\tta-n-é-vet\ttan-évet\n",
      "de-ci-si-on-mak-ing\tde-ci-si-on-mak-ing\tde-ci-si-on-ma-k-ing\tde-ci-si-on-mak-ing\n",
      "fi-deszf-rak-ció\tfi-de-szf-rak-ció\tfi-deszfrak-ció\tfi-desz-frak-ció\n",
      "weeks\twe-eks\twe-eks\tweeks\n",
      "libg-no-me-can-vas\tlibg-no-me-can-vas\tlibg-no-m-e-c-an-vas\tlibg-no-mec-an-vas\n",
      "beson-ders\tbeson-ders\tbe-son-ders\tbeson-ders\n",
      "strengt-he-ned\tstrengt-he-ned\tstrengt-he-n-ed\tstrengt-he-ned\n",
      "goods\tgoods\tgo-ods\tgoods\n",
      "ké-p-ü-ze-net\tké-p-ü-ze-net\tkép-üze-net\tkép-üze-net\n",
      "dist-ri-bu-ci-ón\tdis-t-ri-bu-ci-ón\tdist-ri-bu-ci-ón\tdis-t-ri-bu-ci-ón\n",
      "fi-ve\tfi-ve\tfi-ve\tfive\n",
      "szpon-zo-ri\tszpon-zo-ri\tsz-pon-zo-ri\tszpon-zo-ri\n",
      "el-sö-sor-ban\tel-sö-sor-ban\tel-sö-sor-ban\tel-sö-s-or-ban\n",
      "ch-ris-ti-na\tch-ris-tina\tch-ris-ti-na\tch-ris-tina\n",
      "un-ga-ris-che\tun-ga-ris-c-he\tun-ga-ris-che\tun-ga-ris-che\n",
      "gyászt-va-ló-já-ban\tgyásztva-ló-já-ban\tgyászt-va-ló-já-ban\tgyásztva-ló-já-ban\n",
      "in-ter-ak-ci-ók\tin-te-r-ak-ci-ók\tin-ter-ak-ci-ók\tin-ter-ak-ci-ók\n",
      "hő-söd\thő-s-öd\thő-söd\thő-söd\n",
      "hobby\thob-by\thob-by\thobby\n",
      "ap-prop-ria-te\tapp-rop-ri-a-te\tapp-rop-ria-te\tapp-rop-ria-te\n",
      "ba-lancing\tba-lanc-ing\tba-lan-c-ing\tba-lanc-ing\n",
      "county\tco-unty\tco-unty\tcounty\n",
      "tiem-po\tti-em-po\tti-em-po\tti-em-po\n",
      "fren-tzen\tfren-tzen\tfren-t-zen\tfren-tzen\n",
      "je-ner\tje-ner\tje-ner\tjener\n",
      "párt-ál-lam\tpárt-ál-lam\tpár-t-ál-lam\tpárt-ál-lam\n",
      "whate-ver\twha-t-e-ver\twha-te-ver\twhat-ever\n",
      "hoz-zá-téve\thoz-zá-té-ve\thoz-zá-té-ve\thoz-zá-té-ve\n",
      "ahead\tahe-ad\tahe-ad\tahead\n",
      "ita-li-en\tita-li-en\tita-li-en\titali-en\n",
      "lance\tlan-ce\tlan-ce\tlance\n",
      "hydro-gen\thyd-rogen\thyd-ro-gen\thydrogen\n",
      "lange\tlan-ge\tlan-ge\tlan-ge\n",
      "sch-we-i-tzer\tsch-we-i-tzer\tsch-wei-tzer\tsch-we-i-tzer\n",
      "meoe\tme-oe\tme-oe\tme-oe\n",
      "wales\twales\twa-les\twales\n",
      "cartr-idge\tcart-ridge\tcart-rid-ge\tcart-ridge\n",
      "mous-se\tmous-se\tmo-us-se\tmousse\n",
      "nyersanya-got\tnyers-anya-got\tnyer-s-anya-got\tnyers-anya-got\n",
      "fanyar\tfa-nyar\tfa-nyar\tfa-nyar\n",
      "as-sumpt-ions\tas-sum-pt-ions\tas-sum-pt-ions\tas-sumpt-ions\n",
      "my-self\tmyself\tmy-self\tmyself\n",
      "equi-li-b-ri-um\tequi-lib-ri-um\tequ-i-lib-ri-um\tequi-lib-ri-um\n",
      "in-ju-ri-es\tin-juri-es\tin-ju-ri-es\tin-ju-ri-es\n",
      "szer-teága-zó\tszer-te-á-ga-zó\tszer-te-á-ga-zó\tszer-te-ága-zó\n",
      "ein-he-it\tein-he-it\tein-he-it\tein-heit\n",
      "mi-nu-te\tmi-nu-te\tmi-nu-te\tmi-nute\n",
      "sc-hon\tschon\tschon\tschon\n",
      "ins-tance\tin-s-tance\tins-tance\tins-tance\n",
      "eu-ropj-bio-chem\teu-ro-pj-bio-chem\teu-ropj-bio-chem\teu-ro-pj-bio-chem\n",
      "ei-n-i-gen\tei-ni-gen\tei-n-i-gen\tein-i-gen\n",
      "si-t-e-on\tsi-t-e-on\tsi-te-on\tsi-te-on\n",
      "ré-se-au\tré-se-au\tré-se-au\tré-seau\n",
      "di-rec-to-ra-te\tdi-rec-tora-te\tdi-rec-to-ra-te\tdi-rec-tora-te\n",
      "off-line\toff-line\toff-li-ne\toff-line\n",
      "sur-round\tsur-ro-und\tsur-ro-und\tsur-round\n",
      "un-der-s-tood\tun-der-s-to-od\tun-ders-to-od\tun-ders-to-od\n",
      "suc-c-e-e-ded\tsuc-c-e-e-ded\tsuc-ce-e-ded\tsucc-e-e-ded\n",
      "anyo-ne\tanyone\tanyo-ne\tanyone\n",
      "ralph\tralph\tral-ph\tralph\n",
      "comp-ri-ses\tcom-p-ri-ses\tcomp-ri-ses\tcomp-ri-ses\n",
      "be-uta-ló\tbe-uta-ló\tbe-u-ta-ló\tbe-uta-ló\n",
      "el-derly\tel-derly\tel-der-ly\tel-derly\n",
      "fel-ug-rik\tfel-ug-rik\tfe-l-ug-rik\tfel-ug-rik\n",
      "ab-sch-nitt\tabsch-nitt\tabsch-nitt\tabsch-nitt\n",
      "folk-lore\tfolk-lo-re\tfolk-lo-re\tfolk-lo-re\n",
      "slave\tsla-ve\tsla-ve\tsla-ve\n",
      "hol-mes\thol-mes\thol-mes\tholmes\n",
      "ge-se-hen\tge-se-hen\tge-s-e-hen\tge-se-hen\n",
      "uto-l-ér-te\tutol-ér-te\tutol-ér-te\tutol-ér-te\n",
      "in-st-ru-ments\tinst-ru-ments\tin-st-ru-ments\tinst-ru-ments\n",
      "log-i-cal\tlog-i-cal\tlo-gi-cal\tlog-i-cal\n",
      "fest-mé-nyek\tfest-mé-nyek\tfe-st-mé-nyek\tfest-mé-nyek\n",
      "ano-ny-mus\tano-ny-mus\tanony-mus\tano-ny-mus\n",
      "kí-sér-te-ti-esen\tkí-sér-te-ti-e-sen\tkí-sér-te-ti-e-sen\tkí-sér-te-ti-e-sen\n",
      "know-ledge\tknow-led-ge\tknow-led-ge\tknow-ledge\n",
      "rew-rite\trew-rite\trew-ri-te\trew-rite\n",
      "keep\tke-ep\tke-ep\tkeep\n",
      "com-mu-ni-cating\tcom-mu-ni-cating\tcom-mu-ni-ca-ting\tcom-mu-ni-cating\n",
      "ab-szo-l-út\tab-szo-lút\tab-szo-lút\tab-szo-lút\n",
      "fö-lül\tfö-lül\tfö-lül\tfölül\n",
      "th-re-adelt\tth-re-a-d-elt\tth-re-adelt\tth-re-ad-elt\n",
      "gu-i-val\tgu-i-val\tgui-val\tgui-val\n",
      "ato-mener-gia\tatom-ener-gia\tato-m-e-ner-gia\tatom-ener-gia\n",
      "rod-ri-guez\trod-ri-gu-ez\trod-ri-gu-ez\trod-ri-gu-ez\n",
      "so-x-or\tso-xor\tso-xor\tso-xor\n",
      "or-bá-nék\tor-bá-nék\tor-bá-nék\tor-bán-ék\n",
      "os-car-díj\tos-cardíj\tos-car-díj\tos-cardíj\n",
      "koc-s-mai\tkocs-mai\tko-cs-mai\tkocs-mai\n",
      "vdsz-sz\tvdszsz\tvdsz-sz\tvdsz-sz\n",
      "kon-s-tan-ti-ná-po-lyi\tkons-tan-ti-ná-po-lyi\tkons-tan-ti-ná-po-lyi\tkons-tan-ti-ná-po-lyi\n",
      "pin-nac-le\tpin-nac-le\tpin-n-ac-le\tpin-na-c-le\n",
      "be-st-im-mun-gen\tbe-st-im-mun-gen\tbe-s-tim-mun-gen\tbe-st-im-mun-gen\n",
      "signing\tsign-ing\tsig-n-ing\tsign-ing\n",
      "el-liott\tel-li-ott\tel-li-ott\tel-li-ott\n",
      "na-m-ing\tna-m-ing\tna-ming\tna-ming\n",
      "ezer-szer\tezer-szer\tez-er-szer\tezer-szer\n",
      "pro-cesszek\tpro-ces-szek\tpro-ces-szek\tpro-cesszek\n",
      "jer-ry\tjerry\tjer-ry\tjer-ry\n",
      "köz-sz-fé-rá-ban\tköz-sz-fé-rá-ban\tköz-sz-fé-rá-ban\tköz-szfé-rá-ban\n",
      "ál-la-m-igaz-ga-tás-ban\tál-lam-igaz-ga-tás-ban\tál-lam-igaz-ga-tás-ban\tál-lam-igaz-ga-tás-ban\n",
      "re-sult-ing\tre-sult-ing\tre-sul-t-ing\tre-sult-ing\n",
      "pro-per-ti-es\tpro-per-ti-es\tpro-per-ti-es\tproper-ti-es\n",
      "irá-ny-adók\tirá-ny-adók\tirány-adók\tirány-adók\n",
      "spre-ad-ing\tspre-ad-ing\tspre-a-d-ing\tspread-ing\n",
      "re-cher-che\trecher-che\tre-cher-che\trecher-che\n",
      "tal-al-tam\tta-l-al-tam\tta-l-al-tam\ttal-al-tam\n",
      "mind-ed-dig\tmind-ed-dig\tmin-d-ed-dig\tmind-ed-dig\n",
      "fe-lelős-nek\tfe-le-lős-nek\tfe-le-lős-nek\tfe-le-lős-nek\n",
      "fo-und-ing\tfo-und-ing\tfo-un-d-ing\tfo-und-ing\n",
      "ta-lál-tál\tta-l-ál-tál\tta-lál-tál\tta-lál-tál\n",
      "ki-sis-ko-lá-sok\tkis-is-ko-lá-sok\tki-sis-ko-lá-sok\tkis-is-ko-lá-sok\n",
      "bosz-nia-her-ce-g-o-vi-na\tbosz-ni-a-her-ceg-ovi-na\tbosz-nia-her-ce-go-vi-na\tbosz-nia-her-ce-go-vi-na\n",
      "encrypt-ion\tencrypt-i-on\tencrypt-ion\tencrypt-ion\n",
      "re-pu-lonep\tre-pu-lon-ep\tre-pu-lo-n-ep\tre-pu-lo-nep\n",
      "há-tu-l-ütő-je\thá-tulü-tő-je\thá-tu-lü-tő-je\thá-tul-ütő-je\n",
      "le-ip-zig\tle-ipzig\tle-ip-zig\tle-ip-zig\n",
      "ro-nal-din-ho\tro-n-al-din-ho\tro-nal-din-ho\tro-nal-din-ho\n",
      "ki-lo-m-et-res\tki-lo-met-res\tki-lo-met-res\tki-lo-met-res\n",
      "fran-cia-or-szág\tfran-cia-or-szág\tfran-ci-a-or-szág\tfran-cia-or-szág\n",
      "vizs-ga-idő-szak\tvizs-ga-idő-szak\tvizs-ga-i-dő-szak\tvizs-ga-idő-szak\n",
      "el-unk\telunk\telunk\tel-unk\n",
      "di-g-i-ta-li-zá-lás\tdi-gi-ta-li-zá-lás\tdi-gi-ta-li-zá-lás\tdi-gi-ta-li-zá-lás\n",
      "in-vest-ors\tin-ves-tors\tin-ves-t-ors\tin-ves-tors\n",
      "rep-li-es\tre-p-li-es\trep-li-es\trep-li-es\n",
      "slayers\tslayers\tsla-yers\tslayers\n",
      "of-fi-ces\tof-fices\tof-fi-ces\tof-fices\n",
      "cen-t-e-ná-ri-u-mi\tcen-te-ná-ri-u-mi\tcen-te-ná-ri-u-mi\tcen-te-ná-ri-u-mi\n",
      "nyolc-szor\tnyolc-szor\tnyol-c-szor\tnyolc-szor\n",
      "kö-té-len\tkö-t-é-len\tkö-té-len\tkö-té-len\n",
      "her-me-neu-ti-kai\ther-me-ne-u-ti-kai\ther-me-ne-u-ti-kai\ther-me-ne-u-ti-kai\n",
      "hoz-zász\thoz-zász\thoz-zász\thoz-zá-sz\n",
      "vé-gel-szá-mo-lás\tvé-gel-szá-mo-lás\tvé-g-el-szá-mo-lás\tvég-el-szá-mo-lás\n",
      "peop-le\tpeople\tpe-op-le\tpeople\n",
      "in-vest-ments\tin-vest-ments\tin-vest-ments\tin-vestments\n",
      "popsztár\tpopsztár\tpopsz-tár\tpop-sztár\n",
      "ma-nag-ed\tma-na-g-ed\tma-na-g-ed\tma-nag-ed\n",
      "acpi\tac-pi\tac-pi\tac-pi\n",
      "musort\tmu-sort\tmu-sort\tmusort\n",
      "stu-dying\tstudying\tstu-dying\tstudying\n",
      "cen-zú-ra\tcenz-ú-ra\tcen-zú-ra\tcen-zú-ra\n",
      "en-sured\ten-sur-ed\ten-su-red\ten-sured\n",
      "at-hén-ban\tat-hén-ban\tat-hén-ban\tathén-ban\n",
      "cual-qu-i-er\tcu-al-qu-i-er\tcu-al-qu-i-er\tcu-al-qui-er\n",
      "cre-a-tes\tcre-a-tes\tcrea-tes\tcrea-tes\n",
      "ki-sis-ko-lás\tkisis-ko-lás\tki-sis-ko-lás\tkis-is-ko-lás\n",
      "észak-ko-rea\tészak-ko-r-ea\tészak-ko-rea\tészak-ko-rea\n",
      "room\troom\tro-om\troom\n",
      "ko-má-rome\tko-má-ro-me\tko-má-ro-me\tko-má-ro-me\n",
      "hi-tel-in-té-ze-ti\thi-tel-in-té-ze-ti\thi-te-l-in-té-ze-ti\thi-tel-in-té-ze-ti\n",
      "jus-tice\tjus-ti-ce\tjus-ti-ce\tjus-ti-ce\n",
      "eze-r-éves\teze-r-éves\tezer-éves\tezer-éves\n",
      "jour-na-lists\tjour-na-lists\tjo-ur-na-lists\tjour-na-lists\n",
      "fe-j-ér\tfe-j-ér\tfe-jér\tfej-ér\n",
      "exp-lor-er\texp-lo-rer\texp-lo-rer\texp-lo-rer\n",
      "fa-ye\tfa-ye\tfaye\tfa-ye\n",
      "fre-i-tag\tfre-i-tag\tfre-i-tag\tfre-itag\n",
      "an-zu-wen-den\tanzu-wen-den\tan-zu-wen-den\tan-zu-wen-den\n",
      "au-xi-li-ary\tau-xi-li-ary\tau-xi-li-a-ry\tau-xi-li-ary\n",
      "moun-ta-ins\tmo-un-tains\tmo-un-ta-ins\tmoun-tains\n",
      "el-le-n-ér-té-ke\tel-len-ér-té-ke\tel-len-ér-té-ke\tel-len-ér-té-ke\n",
      "ren-de-zo\tren-d-e-zo\tren-d-ezo\tren-de-zo\n",
      "lamp-son\tlam-p-son\tlamp-son\tlamp-son\n",
      "mais\tma-is\tma-is\tma-is\n",
      "ap-pe-aring\tap-pear-ing\tap-pe-a-r-ing\tap-pe-aring\n",
      "kisem-be-rek\tkis-em-be-rek\tki-sem-be-rek\tkis-em-be-rek\n",
      "gra-phic\tgra-phic\tgra-p-hic\tgra-phic\n",
      "de-i-ne\tde-i-ne\tde-i-ne\tdei-ne\n",
      "ciao\tci-ao\tci-ao\tciao\n",
      "hou-se\tho-u-se\tho-u-se\thou-se\n",
      "mind-egyik-hez\tmind-e-gyik-hez\tmind-egyik-hez\tmind-egyik-hez\n",
      "esta-te\tes-ta-te\tes-ta-te\test-a-te\n",
      "to-p-i-kok\tto-pi-kok\tto-pi-kok\ttop-ikok\n",
      "hiphop\thip-hop\thip-hop\thip-hop\n",
      "mac-do-nald\tmac-do-nald\tmac-do-n-ald\tmac-do-nald\n",
      "und-ren-tide\tund-ren-t-ide\tund-rent-ide\tund-ren-ti-de\n",
      "az-után\tazu-tán\taz-u-tán\taz-után\n",
      "vagyis\tva-gyis\tva-gyis\tvagy-is\n",
      "ge-nyó\tge-nyó\tge-nyó\tgenyó\n",
      "ha-lál-ese-tek\tha-lá-l-ese-tek\tha-lá-l-ese-tek\tha-lál-ese-tek\n",
      "pair\tpair\tpa-ir\tpair\n",
      "fe-e-ling\tfee-ling\tfe-e-ling\tfee-ling\n",
      "kra-usz\tkrausz\tkra-usz\tkra-usz\n",
      "rát-gé-ber\trát-gé-ber\trát-gé-ber\trátgéber\n",
      "passport\tpas-s-port\tpas-sport\tpas-sport\n",
      "as-su-m-ing\tas-su-m-ing\tas-su-m-ing\tas-sum-ing\n",
      "daddy\tdad-dy\tdad-dy\tdaddy\n",
      "sha-r-ing\tsha-ring\tsha-ring\tsha-ring\n",
      "mér-v-adó\tmér-v-a-dó\tmérv-adó\tmérv-adó\n",
      "toth-jo-zsef\ttoth-jo-z-sef\ttoth-jo-zsef\ttoth-jo-zsef\n",
      "co-ope-ra-ti-on\tco-o-pe-ra-ti-on\tco-o-pe-ra-ti-on\tco-ope-ra-ti-on\n",
      "in-st-ru-ment\tinst-ru-ment\tin-st-ru-ment\tinst-ru-ment\n",
      "tin-ta-s-u-ga-ras\ttin-ta-su-ga-ras\ttin-ta-s-u-ga-ras\ttin-ta-su-ga-ras\n",
      "par-tial\tpar-ti-al\tpar-ti-al\tpar-ti-al\n",
      "ve-ge-t-ab-les\tve-ge-tab-les\tve-ge-t-ab-les\tve-ge-tab-les\n",
      "tra-di-ti-on\ttra-di-t-i-on\ttra-di-ti-on\ttra-di-ti-on\n",
      "of-fi-cers\tof-fi-cers\tof-fi-cers\tof-ficers\n",
      "hoz-za-szo-las\thoz-za-szo-l-as\thoz-za-szo-l-as\thoz-za-szol-as\n",
      "pers-pec-ti-ves\tpers-pec-tives\tpers-pec-ti-ves\tpers-pec-ti-ves\n",
      "app-li-cab-le\tapp-li-cab-le\tapp-li-cab-le\tapp-lic-ab-le\n",
      "everyw-he-re\teve-ryw-he-re\teveryw-he-re\teveryw-he-re\n",
      "kub-rick\tkub-rick\tkub-rick\tkubrick\n",
      "me-is-ten\tme-is-ten\tme-is-ten\tmeis-ten\n",
      "leg-a-láb-bis\tleg-a-láb-bis\tle-g-a-láb-bis\tleg-alább-is\n",
      "guin-ness\tgu-in-n-ess\tgu-in-n-ess\tguin-n-ess\n",
      "meg-esik\tme-g-esik\tmeg-esik\tmeg-esik\n",
      "some\tsome\tso-me\tsome\n",
      "vám-pi-ri-zálsz\tvám-pi-ri-zál-sz\tvám-pi-ri-zálsz\tvám-pi-ri-zálsz\n",
      "úris-ten\túris-ten\túris-ten\túr-is-ten\n",
      "la-ti-no-vits\tla-ti-n-o-vits\tla-ti-no-vits\tla-ti-no-vits\n",
      "irá-nyár\tirá-nyár\tirá-nyár\tirány-ár\n",
      "ma-s-ter\tma-s-ter\tmas-ter\tma-s-ter\n",
      "os-bourne\tos-bour-ne\tos-bour-ne\tos-bourne\n",
      "exa-mi-ning\texa-mi-ning\texa-mi-ning\texa-mining\n",
      "mil-lionen\tmil-li-o-nen\tmil-li-o-nen\tmil-li-onen\n",
      "ze-ne-a-ka-dé-mi-án\tze-nea-ka-dé-mi-án\tze-ne-a-ka-dé-mi-án\tze-ne-aka-dé-mi-án\n",
      "un-ga-ris-c-her\tun-ga-ris-cher\tun-ga-ris-cher\tun-ga-ris-cher\n",
      "bűn-ül-dö-zé-si\tbű-n-ül-dö-zé-si\tbűn-ül-dö-zé-si\tbűn-ül-dö-zé-si\n",
      "est-ab-lish\test-ab-lish\tes-t-ab-lish\test-ab-lish\n",
      "ugya-n-is\tugya-n-is\tugya-nis\tugyan-is\n",
      "place\tpla-ce\tpla-ce\tplace\n",
      "an-noun-ce-ments\tan-no-un-ce-ments\tan-no-un-ce-ments\tan-no-un-ce-ments\n",
      "hous-es\tho-us-es\tho-u-s-es\thous-es\n",
      "pi-acké-pes\tpi-ac-ké-pes\tpi-ac-ké-pes\tpi-ac-ké-pes\n",
      "fra-ser\tfra-s-er\tfra-s-er\tfra-ser\n",
      "hi-szé-keny\thiszé-keny\thi-szé-keny\thi-szé-keny\n",
      "ap-pe-a-rance\tap-pe-a-ran-ce\tap-pe-a-ran-ce\tap-pe-arance\n",
      "mel-lék-épü-let-tel\tmel-lék-é-pü-let-tel\tmel-lék-épü-let-tel\tmel-lék-épü-let-tel\n",
      "place-ment\tplace-ment\tpla-ce-ment\tplace-ment\n",
      "rain-bow\tra-in-bow\tra-in-bow\tra-in-bow\n",
      "szé-te-sett\tszé-te-sett\tszé-te-sett\tszét-esett\n",
      "ausdruck\tausd-ruck\tausd-ruck\taus-druck\n",
      "irá-ny-el-ve\tirá-ny-el-ve\tirá-nyel-ve\tirány-el-ve\n",
      "föl-da-lat-ti\tföl-da-lat-ti\tföl-da-lat-ti\tföld-alat-ti\n",
      "bro-ad-way\tbroad-way\tbro-ad-way\tbroad-way\n",
      "lé-gi-autók-kal\tlé-giautók-kal\tlé-gi-a-u-tók-kal\tlé-giautók-kal\n",
      "segu-ri-dad\tse-gu-ri-dad\tse-gu-ri-dad\tse-gu-ri-dad\n",
      "do-l-ores\tdo-lo-r-es\tdo-lo-res\tdo-lor-es\n",
      "isd-n-u-tils\tisd-nu-tils\tisd-nu-tils\tisd-nu-tils\n",
      "kol-lé-gái\tkol-lé-gái\tkol-lé-gá-i\tkol-lé-gái\n",
      "ae-gis\tae-g-is\tae-gis\tae-gis\n",
      "de-pend-ing\tde-pend-ing\tde-pen-ding\tde-pend-ing\n",
      "si-em\tsi-em\tsi-em\tsiem\n",
      "leur\tle-ur\tle-ur\tleur\n",
      "ju-go-szlá-via\tju-go-szlá-via\tju-go-sz-lá-via\tju-go-szlá-via\n",
      "in-jury\tin-jury\tin-ju-ry\tin-jury\n",
      "árvai\tár-vai\tár-vai\tár-vai\n",
      "elit\tel-it\telit\telit\n",
      "unab-le\tun-a-b-le\tun-ab-le\tuna-b-le\n",
      "tried\ttri-ed\ttri-ed\ttried\n",
      "re-make\tre-make\tre-ma-ke\tre-make\n",
      "es-cape\tes-cape\tes-ca-pe\tes-cape\n",
      "vi-sz-lát\tvisz-lát\tvisz-lát\tvisz-lát\n",
      "chang-ed\tchan-g-ed\tchan-ged\tchang-ed\n",
      "bur-den\tbur-den\tbur-den\tburd-en\n",
      "ütésál-ló\tütés-ál-ló\tütés-ál-ló\tütés-ál-ló\n",
      "le-o-pold\tleo-pold\tle-o-pold\tleo-pold\n",
      "sza-bolcs-szat-már-be-reg\tsza-bol-cs-szat-már-be-reg\tsza-bol-cs-szat-már-be-reg\tsza-bolcs-szatmár-be-reg\n",
      "föl-desú-ri\tföl-des-úri\tföl-des-úri\tföl-des-úri\n",
      "ze-ta-jones\tze-ta-jon-es\tze-ta-jo-nes\tzet-ajo-nes\n",
      "me-l-o-day\tme-l-o-day\tme-lo-day\tme-lo-day\n",
      "sér-tege-tő\tsér-te-ge-tő\tsér-te-ge-tő\tsér-te-ge-tő\n",
      "rend-kívü-li\trend-kí-vü-li\trend-kí-vü-li\trend-kí-vü-li\n",
      "szőr-ős\tsző-r-ős\tsző-r-ős\tszőr-ős\n",
      "pél-d-á-ul\tpél-dá-ul\tpél-dá-ul\tpél-dá-ul\n",
      "nyüz-ü-ge\tny-üzü-ge\tnyü-zü-ge\tnyü-zü-ge\n",
      "akk-si\tak-k-si\takk-si\takk-si\n",
      "tex-til-i-pa-ri\ttex-til-ipa-ri\ttex-til-ipa-ri\ttex-til-ipa-ri\n",
      "mindar-ra\tmind-ar-ra\tmin-d-ar-ra\tmind-ar-ra\n",
      "bey-ond\tbeyond\tbeyond\tbeyond\n",
      "hert-ha\thert-ha\thert-ha\ther-tha\n",
      "vi-deot\tvi-de-ot\tvi-de-ot\tvi-deot\n",
      "egye-t-em-ben\tegye-tem-ben\tegye-tem-ben\tegye-tem-ben\n",
      "eu-tel-sat\teut-el-sat\teu-tel-sat\teut-el-sat\n",
      "ho-n-ap\tho-nap\tho-nap\thon-ap\n",
      "de-feat\tde-feat\tde-fe-at\tde-feat\n",
      "on-li-ne\ton-line\ton-li-ne\ton-line\n",
      "basin-ger\tbas-in-ger\tba-s-in-ger\tba-sin-ger\n",
      "jones\tjon-es\tjo-n-es\tjo-nes\n",
      "prin-ci-pale\tprin-c-i-pa-le\tprin-ci-pa-le\tprin-ci-pale\n",
      "va-la-m-i-re-va-ló\tva-la-mi-re-va-ló\tva-la-mi-re-va-ló\tva-la-mi-re-va-ló\n",
      "as-so-ci-a-tions\tas-so-ci-a-tions\tas-so-ci-a-ti-ons\tas-so-ci-a-tions\n",
      "ipar-á-gak\tipar-á-gak\tipar-ágak\tipar-ágak\n",
      "vi-deo-ka-me-ra\tvi-deo-ka-me-ra\tvi-de-o-ka-me-ra\tvi-deo-ka-me-ra\n",
      "fo-reign\tfo-re-i-gn\tfo-re-ign\tfo-rei-gn\n",
      "ma-le\tma-le\tma-le\tmale\n",
      "hear\thear\the-ar\thear\n",
      "po-werpc\tpo-werpc\tpo-wer-pc\tpo-werpc\n",
      "le-b-u-kott\tle-bu-kott\tle-bu-kott\tle-bu-kott\n",
      "ma-in-tained\tma-in-ta-ined\tma-in-t-a-i-ned\tma-in-ta-ined\n",
      "cir-cums-tan-ces\tcir-cumstan-ces\tcir-cum-s-tan-ces\tcir-cum-stan-ces\n",
      "ge-o-dé-zia\tgeo-dé-zia\tge-o-dé-zia\tgeo-dé-zia\n",
      "vagy-is-hogy\tvagy-is-hogy\tva-gy-is-hogy\tvagy-is-hogy\n",
      "be-sze-lunk\tbe-sze-lunk\tbe-sze-lunk\tbesz-elunk\n",
      "ap-pea-red\tap-pea-red\tap-pe-a-red\tap-peared\n",
      "fi-le-o-kat\tfi-leo-kat\tfi-le-o-kat\tfi-leokat\n",
      "cap-ab-le\tca-p-ab-le\tca-p-ab-le\tcap-ab-le\n",
      "betrag\tbe-trag\tbet-rag\tbe-trag\n",
      "félsz\tfél-sz\tfélsz\tfélsz\n",
      "ugyan-az-zal\tugya-n-az-zal\tugyan-az-zal\tugyan-az-zal\n",
      "ag-re-e-ment\tag-ree-ment\tag-re-e-ment\tag-ree-ment\n",
      "se-e-ing\tsee-ing\tse-e-ing\tse-e-ing\n",
      "vers-ionen\tvers-ionen\tver-s-i-o-nen\tvers-ionen\n",
      "cri-te-r-i-on\tcri-te-r-i-on\tcri-te-r-i-on\tcri-ter-ion\n",
      "fábry\tfáb-ry\tfáb-ry\tfábry\n",
      "bur-nett\tburnett\tbur-n-ett\tburnett\n",
      "gent-le-man\tgent-le-man\tgent-le-man\tgentle-man\n",
      "min-den-nek\tmin-den-nek\tmin-den-nek\tminden-nek\n",
      "igazság-hoz\tigaz-ság-hoz\tigaz-ság-hoz\tigaz-ság-hoz\n",
      "het-ven-öt\thet-ve-n-öt\thet-ve-nöt\thet-ven-öt\n",
      "tá-m-a-dók\ttá-m-a-dók\ttá-ma-dók\ttá-ma-dók\n",
      "to-pi-cok\tto-p-i-cok\tto-pi-cok\tto-pi-cok\n",
      "bar-ce-lona\tbar-ce-lo-na\tbar-ce-lo-na\tbar-ce-lo-na\n",
      "com-mand-li-ne\tcom-mand-li-ne\tcom-mand-li-ne\tcom-mand-line\n",
      "fél-é-vé-ben\tfél-é-vé-ben\tfél-évé-ben\tfél-évé-ben\n",
      "be-a-uty\tbe-a-uty\tbe-a-u-ty\tbe-auty\n",
      "du-na-k-eszi\tdu-na-ke-szi\tdu-na-ke-szi\tdu-na-ke-szi\n",
      "te-le-ví-zió-zás-ról\tte-le-ví-zi-ó-zás-ról\tte-le-ví-zi-ó-zás-ról\tte-le-ví-zi-ó-zás-ról\n",
      "view-so-nic\tview-so-nic\tvi-ew-so-nic\tview-so-nic\n",
      "an-che\tan-che\tan-che\tanche\n",
      "as-i-de\tas-ide\tasi-de\tas-i-de\n",
      "ma-in-tainer\tma-in-ta-iner\tma-in-ta-i-ner\tma-in-tainer\n",
      "egy-szer-egy\tegy-szer-egy\tegy-sze-r-egy\tegy-szer-egy\n",
      "wedge\twedge\twed-ge\twedge\n",
      "co-l-ored\tco-l-o-r-ed\tco-lo-red\tcol-or-ed\n",
      "sys-tems\tsys-tems\tsys-t-ems\tsys-tems\n",
      "irá-nyít-sa\tirá-nyít-sa\tirá-nyí-t-sa\tirá-nyít-sa\n",
      "ex-pos-u-re\tex-pos-ure\tex-po-s-u-re\tex-pos-ure\n",
      "fi-che-ros\tfic-he-ros\tfi-che-ros\tfic-he-ros\n",
      "kont-in-gens\tkon-t-in-gens\tkon-t-in-gens\tkon-tin-gens\n",
      "tu-da-ta-lat-ti\ttu-da-ta-lat-ti\ttu-da-ta-lat-ti\ttu-dat-alat-ti\n",
      "iquest\tiquest\tiqu-est\tiquest\n",
      "inf-ra-struk-tú-rá-val\tinf-ra-struk-tú-rá-val\tinf-ra-st-ruk-tú-rá-val\tinf-ra-struk-tú-rá-val\n",
      "fé-l-e-zer\tféle-zer\tfé-l-e-zer\tfél-ezer\n",
      "húgy-úti\thú-gy-úti\thú-gy-ú-ti\thúgy-úti\n",
      "ve-g-e-ta-ti-on\tve-ge-ta-ti-on\tve-ge-ta-ti-on\tve-ge-ta-ti-on\n",
      "ent-re-pri-ses\tent-re-pri-ses\tent-re-p-ri-ses\tent-rep-ri-ses\n",
      "na-ti-o-nal-ism\tna-ti-o-nal-ism\tna-ti-o-na-l-ism\tna-ti-o-nal-ism\n",
      "fel-árért\tfe-l-árért\tfe-lá-rért\tfel-árért\n",
      "szí-vi-zom\tszí-v-i-zom\tszí-vi-zom\tszív-izom\n",
      "tár-s-ra\ttár-s-ra\ttár-s-ra\ttárs-ra\n",
      "ac-qu-is\tac-qu-is\tac-quis\tac-quis\n",
      "ad-du-ser\tad-du-s-er\tad-du-s-er\tad-du-ser\n",
      "in-tegra-tes\tin-te-gra-tes\tin-te-g-ra-tes\tin-te-gra-tes\n",
      "mi-n-u-ten\tmi-nu-ten\tmi-nu-ten\tmi-nuten\n",
      "ez-is\tezis\tezis\tezis\n",
      "er-we-i-tert\ter-wei-tert\ter-we-i-tert\ter-wei-tert\n",
      "holy-field\tholy-fi-eld\tholy-fi-eld\tholy-field\n",
      "the-o-ri-es\tthe-ori-es\tthe-o-ri-es\tthe-ori-es\n",
      "com-pu-ta-ti-on\tcom-p-u-ta-ti-on\tcom-p-u-ta-ti-on\tcomp-uta-ti-on\n",
      "wil-liams\twil-li-ams\twil-li-ams\twil-li-ams\n",
      "sap-p-hi-re\tsap-p-hi-re\tsap-phi-re\tsap-p-hi-re\n",
      "sto-re\tstore\tsto-re\tstore\n",
      "rend-szer-úr\trend-szer-úr\trend-sze-r-úr\trend-szer-úr\n",
      "purp-le\tpurple\tpurp-le\tpurple\n",
      "rep-li-ed\tre-p-li-ed\trep-li-ed\trep-li-ed\n",
      "bak-e-lit\tba-ke-lit\tba-k-e-lit\tba-ke-lit\n",
      "mos-tana-ban\tmos-ta-na-ban\tmos-ta-na-ban\tmos-ta-na-ban\n",
      "gyógy-szer-i-par\tgyógy-szer-ipar\tgyógy-szer-ipar\tgyógy-szer-ipar\n",
      "négy-szö-g-öles\tnégy-szög-öles\tnégy-szö-gö-les\tnégy-szög-öles\n",
      "be-en\tbe-en\tbe-en\tbeen\n",
      "irá-ny-el-ve-it\tirá-nyel-ve-it\tirány-el-ve-it\tirány-el-ve-it\n",
      "ac-cep-tab-le\tac-ceptab-le\tac-cep-t-ab-le\tac-cept-ab-le\n",
      "hydro-car-bons\thydro-car-bons\thyd-ro-car-bons\thydro-car-bons\n",
      "kur-t-ág\tkur-t-ág\tkur-t-ág\tkurt-ág\n",
      "tu-do-mány-egye-tem\ttu-do-mány-egye-tem\ttu-do-má-ny-e-gye-tem\ttu-do-mány-egye-tem\n",
      "vas-utas\tvas-utas\tva-s-u-tas\tvas-utas\n",
      "le-he-to-s-eg\tle-he-tos-eg\tle-he-tos-eg\tle-he-tos-eg\n",
      "trigger-ing\ttri-gger-ing\ttri-g-g-e-r-ing\ttri-gger-ing\n",
      "egér\teg-ér\teg-ér\tegér\n",
      "lég-leg-ze-t-el-ál-lí-tó\tlég-leg-zet-el-ál-lí-tó\tlég-leg-zet-el-ál-lí-tó\tlég-leg-zet-el-ál-lí-tó\n",
      "bear\tbe-ar\tbe-ar\tbear\n",
      "go-n-oszt\tgo-noszt\tgo-noszt\tgo-noszt\n",
      "mu-kö-do\tmu-kö-do\tmu-kö-do\tmu-ködo\n",
      "tol-la-s-labd\ttol-las-labd\ttol-las-labd\ttol-las-labd\n",
      "im-pose\tim-po-se\tim-po-se\tim-po-se\n",
      "mun-ka-meg-osz-tás\tmun-ka-megosz-tás\tmun-ka-meg-osz-tás\tmun-ka-meg-osz-tás\n",
      "ad-ver-t-i-sing\tad-vert-ising\tad-ver-ti-s-ing\tad-vert-ising\n",
      "in-he-r-i-tance\tin-her-i-tance\tin-he-r-i-tance\tin-her-i-tance\n",
      "un-s-tab-le\tun-s-tab-le\tuns-tab-le\tun-stab-le\n",
      "mit-su-b-is-hi\tmit-su-bis-hi\tmit-su-bis-hi\tmi-tsu-bi-shi\n",
      "ma-ri-ah\tma-ri-ah\tma-ri-ah\tma-riah\n",
      "élet-stí-lus\télet-s-tí-lus\télet-s-tí-lus\télet-stí-lus\n",
      "ti-ene\tti-e-ne\tti-e-ne\tti-ene\n",
      "ge-b-aut\tge-baut\tge-b-a-ut\tgeb-aut\n",
      "há-zépí-tés\thá-z-épí-tés\thá-zé-pí-tés\tház-épí-tés\n",
      "rossz-fi-úk\trossz-fi-úk\tros-sz-fi-úk\trossz-fi-úk\n",
      "broad-cast\tbroad-cast\tbro-ad-cast\tbroad-cast\n",
      "gaz-em-ber\tgaz-em-ber\tga-z-em-ber\tgaz-em-ber\n",
      "el-so-re\tel-so-re\tel-so-re\tel-sore\n",
      "déla-me-ri-ka\tdé-lame-ri-ka\tdé-la-me-ri-ka\tdél-ame-ri-ka\n",
      "fe-el\tfe-el\tfe-el\tfeel\n",
      "phases\tpha-s-es\tpha-s-es\tphas-es\n",
      "eu-ta-ná-zia\te-u-ta-ná-zia\teu-ta-ná-zia\teu-ta-ná-zia\n",
      "all-ge-me-i-ne\tall-g-e-me-i-ne\tall-ge-me-i-ne\tall-ge-mei-ne\n",
      "vane\tva-ne\tva-ne\tva-ne\n",
      "uta-s-tér-ben\tutas-tér-ben\tutas-tér-ben\tutas-tér-ben\n",
      "si-eg-ra-vec-le\tsieg-ra-vec-le\tsi-eg-ra-vec-le\tsieg-ra-ve-cle\n",
      "fran-cia-or-szá-gi\tfran-cia-or-szá-gi\tfran-ci-a-or-szá-gi\tfran-cia-or-szá-gi\n",
      "free-ware\tfree-ware\tfree-wa-re\tfree-ware\n",
      "whilst\twhil-st\twhil-st\twhil-st\n",
      "se-u-le-ment\tseu-le-ment\tse-u-le-ment\tse-u-le-ment\n",
      "cle-a-ring\tcle-ar-ing\tcle-a-ring\tcle-a-ring\n",
      "test-től\ttest-től\tte-st-től\ttest-től\n",
      "que-ri-es\tqu-e-ri-es\tque-ri-es\tque-ri-es\n",
      "usals\tus-als\tusals\tusals\n",
      "wearing\twe-aring\twea-ring\twe-a-ring\n",
      "gim-na-zis-ta\tgim-na-z-is-ta\tgim-na-zis-ta\tgim-na-zis-ta\n",
      "pá-rizs-ba\tpá-rizs-ba\tpá-ri-zs-ba\tpá-rizs-ba\n",
      "lions\tlions\tli-ons\tlions\n",
      "po-verty\tpo-verty\tpo-ver-ty\tpo-verty\n",
      "hü-ly-egye-re-kek\thü-ly-egye-re-kek\thü-lye-gye-re-kek\thü-ly-egye-re-kek\n",
      "planes\tpla-nes\tpla-nes\tplanes\n",
      "le-ü-té-si\tle-ü-té-si\tle-üté-si\tle-üté-si\n",
      "szárny\tszárny\tszár-ny\tszárny\n",
      "app-ro-a-c-hes\tapp-ro-a-ches\tapp-ro-a-ches\tapp-ro-a-ches\n",
      "si-ke-rult\tsi-ke-rult\tsi-ke-rult\tsi-kerult\n",
      "gé-néral\tgé-né-ral\tgé-né-ral\tgé-né-ral\n",
      "ve-ga-son\tve-ga-son\tve-g-a-son\tveg-a-son\n",
      "me-ne-dzsel-ni\tme-ne-dzsel-ni\tme-ne-dzs-el-ni\tme-ne-dzsel-ni\n",
      "ma-na-gers\tma-na-g-ers\tma-na-gers\tma-na-gers\n",
      "sour-ce-for-ge-on\tsour-ce-for-ge-on\tsour-ce-for-ge-on\tsour-ce-for-geon\n",
      "va-lor\tva-l-or\tva-lor\tva-lor\n",
      "zár-szá-ma-dás\tzár-szá-ma-dás\tzár-szá-ma-dás\tzár-szám-adás\n",
      "pe-pe\tpe-pe\tpe-pe\tpepe\n",
      "sosincs\tsosincs\tso-sincs\tso-sincs\n",
      "stra-teg-ic\tstra-te-gic\tstra-te-gic\tstrategic\n",
      "metals\tme-tals\tme-tals\tme-tals\n",
      "countrys\tcount-rys\tcount-rys\tcount-rys\n",
      "sugg-es-ted\tsugg-es-ted\tsug-ges-ted\tsugg-es-ted\n",
      "hun-gary-net-work\thun-gary-net-work\thun-gary-net-work\thun-garynet-work\n",
      "oc-cur\toc-c-ur\toc-cur\toc-cur\n",
      "cold-p-lay\tcold-p-lay\tcold-play\tcold-play\n",
      "lé-nyeg-i-leg\tlé-nye-gi-leg\tlé-nye-gi-leg\tlé-nye-gi-leg\n",
      "szap-pan-ope-ra\tszap-pan-ope-ra\tszap-pa-n-ope-ra\tszap-pan-ope-ra\n",
      "or-ga-ni-sa-tions\tor-ga-ni-sa-tions\tor-ga-ni-sa-ti-ons\tor-ga-ni-sa-tions\n",
      "fe-l-éb-red\tfe-l-éb-red\tfel-éb-red\tfel-éb-red\n",
      "fil-ma-ka-dé-mia\tfil-ma-ka-dé-mia\tfil-ma-ka-dé-mia\tfilm-aka-dé-mia\n",
      "alone\talone\talo-ne\talo-ne\n",
      "dal-ebgkris-tály-vér-tet\tdal-ebg-kris-tály-vér-tet\tdal-ebgkris-tály-vér-tet\tdal-ebg-kris-tály-vér-tet\n",
      "kan-cel-lár\tkan-c-el-lár\tkan-cel-lár\tkan-cel-lár\n",
      "col-l-e-ge\tcol-l-e-ge\tcol-le-ge\tcol-l-e-ge\n",
      "lét-szám-le-épí-tés\tlét-szám-le-é-pí-tés\tlét-szám-le-épí-tés\tlét-szám-le-épí-tés\n",
      "maest-ro\tma-est-ro\tma-e-st-ro\tmaest-ro\n",
      "pre-sen-ta-ti-on\tpre-s-en-ta-ti-on\tpre-sen-ta-ti-on\tpre-s-en-ta-ti-on\n",
      "bu-da-pes-ter\tbu-da-pes-ter\tbu-da-pes-ter\tbu-da-pest-er\n",
      "yi-elds\tyi-elds\tyi-elds\tyields\n",
      "so-c-ket\tso-c-ket\tsoc-ket\tso-cket\n",
      "won-d-er\twon-der\twon-d-er\twon-der\n",
      "epide-mio-ló-gi-ai\tep-i-de-mio-ló-gi-ai\tepi-de-mio-ló-gi-ai\tepi-de-mio-ló-gi-ai\n",
      "lap-ki-adó\tlap-ki-adó\tlap-ki-a-dó\tlap-ki-adó\n",
      "geo-ter-mi-kus\tgeo-ter-mi-kus\tge-o-ter-mi-kus\tgeo-ter-mi-kus\n",
      "kom-mu-ni-ka-ti-on\tkom-mu-ni-ka-t-i-on\tkom-mu-ni-ka-ti-on\tkom-mu-ni-kat-ion\n",
      "fel-épült\tfel-épült\tfel-é-pült\tfel-épült\n",
      "me-leg-víz\tme-l-eg-víz\tme-leg-víz\tme-leg-víz\n",
      "eagle\teag-le\teag-le\teag-le\n",
      "szk-ript\tszk-ript\tszk-ri-pt\tszkript\n",
      "da-ta-p-ro-ces-sing\tda-ta-p-ro-ces-sing\tda-tap-ro-ces-sing\tda-ta-pro-ces-sing\n",
      "stuck\tstuck\tstuck\tstu-ck\n",
      "hi-á-ny-o-lom\thi-á-nyo-lom\thi-á-nyo-lom\thi-á-nyo-lom\n",
      "ma-inst-ream\tma-inst-ream\tma-in-st-ream\tma-inst-ream\n",
      "berns-te-in\tberns-tein\tberns-te-in\tberns-tein\n",
      "mon-deo\tmon-d-eo\tmon-deo\tmon-deo\n",
      "sake\tsake\tsa-ke\tsake\n",
      "wich-tig-s-ten\twich-tig-sten\twich-tig-sten\twich-tig-sten\n",
      "mö-g-ül\tmö-gül\tmö-gül\tmö-gül\n",
      "bun-d-es-li-ga\tbun-des-li-ga\tbun-des-li-ga\tbun-des-li-ga\n",
      "egy-be-esik\tegy-be-esik\tegy-be-e-sik\tegy-be-esik\n",
      "bas-e-don-bass\tba-s-e-don-bass\tba-s-e-don-bass\tbas-edon-bass\n",
      "lá-zadást\tlá-za-dást\tlá-za-dást\tlá-za-dást\n",
      "kö-ben-havn\tköb-en-havn\tkö-ben-havn\tköb-en-havn\n",
      "vil-la-g-es\tvil-la-ges\tvil-la-ges\tvil-la-ges\n",
      "wa-ren\twa-ren\twa-ren\twaren\n",
      "hor-dere-jű\thor-de-re-jű\thor-d-er-e-jű\thord-ere-jű\n",
      "zo-lee\tzo-lee\tzo-lee\tzol-ee\n",
      "exc-han-ges\texc-han-ges\texc-han-ges\texc-hanges\n",
      "hun-ga-ro-r-in-gen\thun-ga-ror-in-gen\thun-ga-ro-r-in-gen\thun-ga-ror-in-gen\n",
      "he-at-ing\theat-ing\thea-t-ing\theat-ing\n",
      "ka-lau-zol-ja\tka-la-u-zol-ja\tka-la-u-zol-ja\tka-la-u-zol-ja\n",
      "péc-s-vá-rad\tpécs-vá-rad\tpécs-vá-rad\tpécs-vá-rad\n",
      "pro-bal-tam\tpro-bal-tam\tpro-b-al-tam\tpro-balt-am\n",
      "dia-lo-gue\tdia-lo-gue\tdi-a-lo-gue\tdia-lo-gue\n",
      "apo-ka-lip-szis\tapo-k-a-lip-szis\tapo-ka-lip-szis\tapo-ka-lip-szis\n",
      "el-va-rá-zsolt\tel-va-rá-z-solt\tel-va-rá-zsolt\tel-va-rá-zsolt\n",
      "ar-gen-t-i-na\tar-gen-ti-na\tar-gen-ti-na\tar-gen-ti-na\n",
      "zwis-c-hen\tzwis-chen\tzwis-chen\tzwis-c-hen\n",
      "pon-te\tpon-te\tpon-te\tponte\n",
      "szám-űzött\tszám-űzött\tszá-m-ű-zött\tszám-űzött\n",
      "gra-vity\tgra-vi-ty\tgra-vity\tgra-vity\n",
      "pletyká-kat\tplety-ká-kat\tplety-ká-kat\tplety-ká-kat\n",
      "tech-no-log-i-cal\ttech-no-log-i-cal\ttech-no-lo-g-i-cal\ttech-no-log-i-cal\n",
      "műt-rá-gya\tműtr-á-gya\tműt-rá-gya\tmű-trá-gya\n",
      "két-ü-lé-ses\tkét-ü-lé-ses\tkét-ü-lé-ses\tkét-ülé-ses\n",
      "tung-sram\ttungs-ram\ttung-sram\ttungs-ram\n",
      "ge-en\tge-en\tge-en\tgeen\n",
      "fe-ren-czi\tfe-ren-c-zi\tfe-ren-czi\tfe-ren-czi\n",
      "korty\tkorty\tkor-ty\tkorty\n",
      "sze-re-p-osz-tás\tsze-re-p-osz-tás\tsze-rep-osz-tás\tsze-rep-osz-tás\n",
      "ma-gyar-or-szág-ra\tma-gyar-or-szág-ra\tma-gya-r-or-szág-ra\tma-gyar-or-szág-ra\n",
      "rea-son\tre-a-son\tre-a-son\trea-son\n",
      "lan-gu-age\tlan-gu-a-ge\tlan-gu-age\tlan-gu-age\n",
      "freu-de\tfre-u-de\tfre-u-de\tfreu-de\n",
      "li-be-rá-l-i-sok\tli-be-rá-li-sok\tli-be-rá-li-sok\tli-be-rá-li-sok\n",
      "rész-fel-ada-tok\trész-fel-ada-tok\trész-fe-l-ada-tok\trész-fel-ada-tok\n",
      "belole\tbe-l-o-le\tbe-l-o-le\tbel-ole\n",
      "egy-sze-rü-en\tegy-sze-r-ü-en\tegy-sze-rü-en\tegy-sze-rü-en\n",
      "átül-tet-ni\tát-ül-tet-ni\tát-ül-tet-ni\tát-ül-tet-ni\n",
      "ei-nen\tei-nen\tei-n-en\tei-nen\n",
      "ec-ho\techo\tec-ho\techo\n",
      "éle-te-rő\téle-te-rő\téle-te-rő\télet-erő\n",
      "ál-la-t-e-gész-ség-ügyi\tál-la-te-gész-ség-ügyi\tál-la-te-gész-ség-ügyi\tál-lat-egész-ség-ügyi\n",
      "hí-r-ös\thír-ös\thí-rös\thír-ös\n",
      "le-he-tose-get\tle-he-tos-eget\tle-he-tos-eget\tle-he-tos-eget\n",
      "eg-é-szé-nek\tegé-szé-nek\tegé-szé-nek\tegé-szé-nek\n",
      "in-za-g-hi\tin-zag-hi\tin-za-g-hi\tin-zag-hi\n",
      "el-ső-éves\tel-ső-éves\tel-ső-é-ves\tel-ső-éves\n",
      "szlogen-nel\tsz-lo-gen-nel\tszlo-gen-nel\tszlo-gen-nel\n",
      "co-e-t-zee\tco-et-zee\tco-et-zee\tco-et-zee\n",
      "re-le-asezel\tre-le-ase-zel\tre-le-as-e-zel\tre-le-as-e-zel\n",
      "ezek-hez\tezek-hez\tez-ek-hez\tezek-hez\n",
      "weil\tweil\twe-il\twe-il\n",
      "di-sab-le\tdi-s-ab-le\tdi-sab-le\tdis-ab-le\n",
      "ab-lak-elől\tab-lak-e-lől\tab-lak-elől\tab-lak-elől\n",
      "kunst\tkunst\tkun-st\tkun-st\n",
      "properly\tproperly\tpro-perly\tproperly\n",
      "na-gyará-nyú\tna-gy-ará-nyú\tna-gy-ará-nyú\tnagy-ará-nyú\n",
      "el-ect-ri-cal\telect-ri-cal\tel-ect-ri-cal\tel-ectri-cal\n",
      "ver-seghy\tver-seghy\tver-seghy\tver-se-ghy\n",
      "la-u-de\tla-u-de\tla-ude\tla-ude\n",
      "ko-z-igaz-ga-tas\tkoz-igaz-ga-tas\tko-z-igaz-ga-tas\tkoz-igaz-ga-tas\n",
      "hol-m-it\thol-mit\thol-mit\thol-mit\n",
      "köz-szfé-ra\tköz-szfé-ra\tköz-sz-fé-ra\tköz-szfé-ra\n",
      "tö-meg-es\ttö-me-g-es\ttö-me-ges\ttö-me-ges\n",
      "he-a-l-ing\the-a-l-ing\the-a-ling\the-a-ling\n",
      "edge\ted-ge\ted-ge\tedge\n",
      "jót-ál-lá-si\tjót-ál-lá-si\tjó-t-ál-lá-si\tjót-ál-lá-si\n",
      "kit-c-hen\tkit-chen\tkit-chen\tkit-chen\n",
      "te-il-wei-se\tte-il-wei-se\tte-il-we-i-se\tte-il-wei-se\n",
      "respects\tres-pects\tres-pects\trespects\n",
      "irá-ny-elv\tirá-ny-elv\tirá-nyelv\tirány-elv\n",
      "he-ly-nök\thely-nök\thely-nök\thely-nök\n",
      "vi-deo-zás\tvi-deo-zás\tvi-de-o-zás\tvi-de-o-zás\n",
      "be-st-im-men\tbe-stim-men\tbe-stim-men\tbe-stim-men\n",
      "kvan-ti-ta-tív\tkvan-t-i-ta-tív\tkvan-ti-ta-tív\tkvan-ti-ta-tív\n",
      "es-tán\tes-tán\tes-tán\test-án\n",
      "hap-py\thap-py\thappy\thap-py\n",
      "ha-t-e-zer\that-e-zer\tha-t-e-zer\that-ezer\n",
      "spie-gel\tspi-e-gel\tspi-e-gel\tspie-gel\n",
      "pre-mi-ses\tpre-mi-ses\tpre-mi-s-es\tpre-mi-ses\n",
      "qua-ke\tqu-a-ke\tqu-a-ke\tqu-ake\n",
      "win-c-hes-ter\twin-ches-ter\twin-ches-ter\twin-ches-ter\n",
      "jo-val\tjo-val\tjo-val\tjoval\n",
      "másutt\tmá-s-utt\tmá-sutt\tmá-sutt\n",
      "ca-pa-ci-ty\tca-pa-ci-ty\tca-pa-city\tca-pa-ci-ty\n",
      "összér-té-ke\tös-szér-té-ke\tös-szér-té-ke\tössz-ér-té-ke\n",
      "peg-asus\tpe-g-a-sus\tpe-ga-sus\tpe-ga-sus\n",
      "pow-der\tpow-der\tpow-d-er\tpow-der\n",
      "keser-nyés\tke-ser-nyés\tke-ser-nyés\tke-ser-nyés\n",
      "mi-nu-tes\tmi-nu-tes\tmi-nu-tes\tmi-nutes\n",
      "player\tpla-yer\tpla-yer\tplayer\n",
      "mér-t-a-ni\tmér-ta-ni\tmér-ta-ni\tmér-ta-ni\n",
      "el-érés-sel\tel-é-rés-sel\tel-érés-sel\tel-érés-sel\n",
      "hús-v-ér\thús-vér\thús-vér\thús-vér\n",
      "paál\tpa-ál\tpa-ál\tpaál\n",
      "vizs-ga-idő-szak-ban\tvizs-ga-idő-szak-ban\tvizs-ga-i-dő-szak-ban\tvizs-ga-idő-szak-ban\n",
      "pres-t-ige\tpre-s-ti-ge\tprest-ige\tprest-ige\n",
      "fi-chi-er\tfi-ch-i-er\tfi-chi-er\tfic-hi-er\n",
      "ca-pi-tal\tca-p-i-tal\tca-pi-tal\tca-p-ital\n",
      "avo-i-ded\tavo-i-ded\tavo-i-ded\tavo-id-ed\n",
      "szent-írást\tszent-í-rást\tszen-tí-rást\tszent-írást\n",
      "au-dio-top\tau-dio-top\tau-di-o-top\tau-dio-top\n",
      "trac-king\ttrac-king\ttrac-k-ing\ttrac-king\n",
      "mail-box\tmail-box\tma-il-box\tmail-box\n",
      "mitg-li-eder\tmit-g-li-eder\tmitg-li-eder\tmit-g-li-eder\n",
      "so-ós\tso-ós\tso-ós\tsoós\n",
      "le-lé-pett\tle-lépett\tle-lé-pett\tle-lé-pett\n",
      "tal-p-ra\ttal-p-ra\ttalp-ra\ttalp-ra\n",
      "irá-ny-adó\tirá-ny-adó\tirá-ny-a-dó\tirány-adó\n",
      "vá-má-ru\tvá-má-ru\tvá-má-ru\tvám-áru\n",
      "alis-tair\tal-is-tair\tal-is-ta-ir\tal-is-tair\n",
      "cha-rac-te-r-is-tic\tcha-rac-te-ris-tic\tcha-rac-te-ris-tic\tcha-rac-te-r-is-tic\n",
      "pro-bal-ja\tpro-b-al-ja\tpro-b-al-ja\tprob-al-ja\n",
      "ga-laxy\tga-laxy\tga-la-xy\tga-laxy\n",
      "fo-cus-es\tfo-cu-s-es\tfo-cu-s-es\tfo-cus-es\n",
      "hó-e-sés\thó-e-sés\thó-e-sés\thó-esés\n",
      "cor-rupt-ion\tcor-rupt-ion\tcor-rupt-i-on\tcor-rupt-ion\n",
      "roo-se-velt\troo-se-velt\tro-o-se-velt\troo-se-velt\n",
      "conso-le\tcon-s-o-le\tcon-so-le\tcons-o-le\n",
      "be-i-ga-zo-ló-dott\tbe-i-ga-zo-ló-dott\tbe-i-ga-zo-ló-dott\tbe-iga-zo-ló-dott\n",
      "tar-kovsz-kij\ttar-kovsz-kij\ttar-kov-sz-kij\ttar-kovsz-kij\n",
      "el-e-na\tele-na\tel-e-na\tel-ena\n",
      "abu-sive\tab-u-s-ive\tabu-s-ive\tabu-sive\n",
      "leg-rand\tleg-rand\tleg-rand\tlegrand\n",
      "email-ci-me\tema-il-ci-me\tema-il-ci-me\tema-il-ci-me\n",
      "el-liot\tel-li-ot\tel-li-ot\tel-li-ot\n",
      "di-cer-i-doo\tdi-ce-r-i-doo\tdi-ce-r-i-doo\tdi-cer-i-doo\n",
      "co-log-ne\tco-log-ne\tco-logne\tco-log-ne\n",
      "to-le-rance\tto-l-e-rance\tto-le-rance\tto-le-rance\n",
      "sió-fok\tsió-fok\tsi-ó-fok\tsió-fok\n",
      "succes-si-on\tsucc-es-si-on\tsucc-es-si-on\tsucc-es-si-on\n",
      "fel-ese-gem\tfel-ese-gem\tfe-l-e-se-gem\tfel-ese-gem\n",
      "epi-so-de\tepis-o-de\tep-i-so-de\tepi-sode\n",
      "ugyan-ez\tugyan-ez\tugya-n-ez\tugyan-ez\n",
      "widget\twid-get\twid-get\twidget\n",
      "eg-é-szé-ben\tegé-szé-ben\tegé-szé-ben\tegé-szé-ben\n",
      "há-ny-in-ger\thány-in-ger\thá-ny-in-ger\thány-in-ger\n"
     ]
    }
   ],
   "source": [
    "hypher = pyphen.Pyphen(lang='hu_HU')\n",
    "\n",
    "N = -1\n",
    "print(\"FFNN\\tCNN\\tLSTM\\tTarget\")\n",
    "for i in range(len(tests_words[0:N])):\n",
    "    hyphs = \"\"\n",
    "    test_word = tests_words[i,0]\n",
    "    test_ic = tests_input_cnn[i:i+1]\n",
    "    test_tags = hyph_predict(test_word, model_dnn,\n",
    "                             window_length, length_after,\n",
    "                             tag_chars,\n",
    "                             model_type=\"dnn\")\n",
    "    ffnn = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=ffnn+'\\t'\n",
    "    \n",
    "    test_result = model_cnn.predict(test_ic)\n",
    "    test_result = test_result[0]\n",
    "    test_tags = result_decode(test_result)\n",
    "    \n",
    "    cnn = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=cnn+'\\t'\n",
    "    \n",
    "    test_result = model_lstm.predict(test_ic)\n",
    "    test_result = test_result[0]\n",
    "    test_tags = result_decode(test_result)\n",
    "    \n",
    "    lstm = hyp_inserted(test_word,test_tags)\n",
    "    hyphs +=lstm+'\\t'\n",
    "    \n",
    "    target = hypher.inserted(test_word) \n",
    "    hyphs += target\n",
    "    if (ffnn!=target) or (cnn!=target) or (lstm!=target):\n",
    "        print(hyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-386060583b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhardmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bc736ad03449>\u001b[0m in \u001b[0;36mone_hot_encode\u001b[0;34m(char, dictionary)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BMES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "one_hot_encode(hardmax(test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61187395,  0.66373782,  0.82366587,  0.89691012,  0.51733389])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
